{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, csv file I/O (e.g. pd.read_csv)\n",
    "import os # deal with os primitives\n",
    "import time # time-related functions\n",
    "import matplotlib.pyplot as plt # plots\n",
    "import pickle # object serialization\n",
    "from collections import Counter # dict subclass for counting hashable items\n",
    "from tqdm import tqdm # iterable object\n",
    "import random # random values generator\n",
    "import seaborn as sns # prettier plots\n",
    "import torch # main package for PyTorch\n",
    "import torch.utils.data as data_utils # access data sets, including pre-processing, loading, and splitting\n",
    "from torch.utils.data import random_split # randomly split a dataset \n",
    "import torch.optim as optim # optimization algorithms\n",
    "import torch.nn as nn # build neural network (layers, activations, loss functions)\n",
    "import torch.nn.functional as F # functions used to build neural network\n",
    "from torchsummary import summary # print the summary of a neural network model\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts # scheduler used to adjust the learning rate\n",
    "from torch.utils.tensorboard.writer import SummaryWriter # nn log writer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "torch.manual_seed(1111)\n",
    "torch.cuda.manual_seed(1111)\n",
    "np.random.seed(1111)\n",
    "random.seed(1111)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>ratio_n_m</th>\n",
       "      <th>ratio_m_n</th>\n",
       "      <th>density</th>\n",
       "      <th>min_degree</th>\n",
       "      <th>max_degree</th>\n",
       "      <th>mean_degree</th>\n",
       "      <th>median_degree</th>\n",
       "      <th>q0.25_degree</th>\n",
       "      <th>q0.75_degree</th>\n",
       "      <th>variation_coefficient_degree</th>\n",
       "      <th>entropy_degree</th>\n",
       "      <th>best_alg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3231</td>\n",
       "      <td>6094</td>\n",
       "      <td>0.530194</td>\n",
       "      <td>1.886103</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.772207</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.111964</td>\n",
       "      <td>0.783043</td>\n",
       "      <td>astar_alg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1129</td>\n",
       "      <td>53906</td>\n",
       "      <td>0.020944</td>\n",
       "      <td>47.746678</td>\n",
       "      <td>0.084657</td>\n",
       "      <td>65</td>\n",
       "      <td>122</td>\n",
       "      <td>95.493357</td>\n",
       "      <td>96.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.096467</td>\n",
       "      <td>5.217304</td>\n",
       "      <td>dfs_alg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4100</td>\n",
       "      <td>1254237</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>305.911463</td>\n",
       "      <td>0.149262</td>\n",
       "      <td>538</td>\n",
       "      <td>694</td>\n",
       "      <td>611.822927</td>\n",
       "      <td>611.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>0.036566</td>\n",
       "      <td>6.499606</td>\n",
       "      <td>dfs_alg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4277</td>\n",
       "      <td>12831</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>astar_alg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4712</td>\n",
       "      <td>9286</td>\n",
       "      <td>0.507431</td>\n",
       "      <td>1.970713</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.941426</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.060489</td>\n",
       "      <td>0.324730</td>\n",
       "      <td>dfs_alg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5330</th>\n",
       "      <td>1874</td>\n",
       "      <td>243966</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>130.184632</td>\n",
       "      <td>0.139012</td>\n",
       "      <td>211</td>\n",
       "      <td>311</td>\n",
       "      <td>260.369264</td>\n",
       "      <td>261.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>0.056847</td>\n",
       "      <td>5.890843</td>\n",
       "      <td>astar_alg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>1843</td>\n",
       "      <td>3686</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>astar_alg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332</th>\n",
       "      <td>3349</td>\n",
       "      <td>6698</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>astar_alg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5333</th>\n",
       "      <td>3317</td>\n",
       "      <td>429198</td>\n",
       "      <td>0.007728</td>\n",
       "      <td>129.393428</td>\n",
       "      <td>0.078042</td>\n",
       "      <td>210</td>\n",
       "      <td>317</td>\n",
       "      <td>258.786856</td>\n",
       "      <td>259.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0.058965</td>\n",
       "      <td>5.952970</td>\n",
       "      <td>astar_alg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334</th>\n",
       "      <td>4849</td>\n",
       "      <td>998689</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>205.957723</td>\n",
       "      <td>0.084966</td>\n",
       "      <td>342</td>\n",
       "      <td>494</td>\n",
       "      <td>411.915446</td>\n",
       "      <td>412.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>0.047067</td>\n",
       "      <td>6.300644</td>\n",
       "      <td>astar_alg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5335 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_nodes  num_edges  ratio_n_m   ratio_m_n   density  min_degree  \\\n",
       "0          3231       6094   0.530194    1.886103  0.001168           2   \n",
       "1          1129      53906   0.020944   47.746678  0.084657          65   \n",
       "2          4100    1254237   0.003269  305.911463  0.149262         538   \n",
       "3          4277      12831   0.333333    3.000000  0.001403           6   \n",
       "4          4712       9286   0.507431    1.970713  0.000837           2   \n",
       "...         ...        ...        ...         ...       ...         ...   \n",
       "5330       1874     243966   0.007681  130.184632  0.139012         211   \n",
       "5331       1843       3686   0.500000    2.000000  0.002172           4   \n",
       "5332       3349       6698   0.500000    2.000000  0.001195           4   \n",
       "5333       3317     429198   0.007728  129.393428  0.078042         210   \n",
       "5334       4849     998689   0.004855  205.957723  0.084966         342   \n",
       "\n",
       "      max_degree  mean_degree  median_degree  q0.25_degree  q0.75_degree  \\\n",
       "0              4     3.772207            4.0           4.0           4.0   \n",
       "1            122    95.493357           96.0          89.0         102.0   \n",
       "2            694   611.822927          611.0         597.0         627.0   \n",
       "3              6     6.000000            6.0           6.0           6.0   \n",
       "4              4     3.941426            4.0           4.0           4.0   \n",
       "...          ...          ...            ...           ...           ...   \n",
       "5330         311   260.369264          261.0         250.0         271.0   \n",
       "5331           4     4.000000            4.0           4.0           4.0   \n",
       "5332           4     4.000000            4.0           4.0           4.0   \n",
       "5333         317   258.786856          259.0         248.0         269.0   \n",
       "5334         494   411.915446          412.0         399.0         425.0   \n",
       "\n",
       "      variation_coefficient_degree  entropy_degree   best_alg  \n",
       "0                         0.111964        0.783043  astar_alg  \n",
       "1                         0.096467        5.217304    dfs_alg  \n",
       "2                         0.036566        6.499606    dfs_alg  \n",
       "3                         0.000000       -0.000000  astar_alg  \n",
       "4                         0.060489        0.324730    dfs_alg  \n",
       "...                            ...             ...        ...  \n",
       "5330                      0.056847        5.890843  astar_alg  \n",
       "5331                      0.000000       -0.000000  astar_alg  \n",
       "5332                      0.000000       -0.000000  astar_alg  \n",
       "5333                      0.058965        5.952970  astar_alg  \n",
       "5334                      0.047067        6.300644  astar_alg  \n",
       "\n",
       "[5335 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./data/df_train.csv\")\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mapping for 'best_alg':\n",
      "astar_alg: 0\n",
      "bfs_alg: 1\n",
      "dfs_alg: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_best_alg = LabelEncoder()\n",
    "\n",
    "# Encode 'best_alg'\n",
    "df_train['best_alg_encoded'] = le_best_alg.fit_transform(df_train['best_alg'])\n",
    "\n",
    "# Print the mapping for 'best_alg'\n",
    "print(\"\\nMapping for 'best_alg':\")\n",
    "for original, encoded in zip(le_best_alg.classes_, le_best_alg.transform(le_best_alg.classes_)):\n",
    "    print(f\"{original}: {encoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['best_alg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_nodes', 'num_edges', 'ratio_n_m', 'ratio_m_n', 'density',\n",
       "       'min_degree', 'max_degree', 'mean_degree', 'median_degree',\n",
       "       'q0.25_degree', 'q0.75_degree', 'variation_coefficient_degree',\n",
       "       'entropy_degree', 'best_alg_encoded'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>ratio_n_m</th>\n",
       "      <th>ratio_m_n</th>\n",
       "      <th>density</th>\n",
       "      <th>min_degree</th>\n",
       "      <th>max_degree</th>\n",
       "      <th>mean_degree</th>\n",
       "      <th>median_degree</th>\n",
       "      <th>q0.25_degree</th>\n",
       "      <th>q0.75_degree</th>\n",
       "      <th>variation_coefficient_degree</th>\n",
       "      <th>entropy_degree</th>\n",
       "      <th>best_alg_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3231</td>\n",
       "      <td>6094</td>\n",
       "      <td>0.530194</td>\n",
       "      <td>1.886103</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.772207</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.111964</td>\n",
       "      <td>0.783043</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1129</td>\n",
       "      <td>53906</td>\n",
       "      <td>0.020944</td>\n",
       "      <td>47.746678</td>\n",
       "      <td>0.084657</td>\n",
       "      <td>65</td>\n",
       "      <td>122</td>\n",
       "      <td>95.493357</td>\n",
       "      <td>96.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.096467</td>\n",
       "      <td>5.217304</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4100</td>\n",
       "      <td>1254237</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>305.911463</td>\n",
       "      <td>0.149262</td>\n",
       "      <td>538</td>\n",
       "      <td>694</td>\n",
       "      <td>611.822927</td>\n",
       "      <td>611.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>0.036566</td>\n",
       "      <td>6.499606</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4277</td>\n",
       "      <td>12831</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4712</td>\n",
       "      <td>9286</td>\n",
       "      <td>0.507431</td>\n",
       "      <td>1.970713</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3.941426</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.060489</td>\n",
       "      <td>0.324730</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5330</th>\n",
       "      <td>1874</td>\n",
       "      <td>243966</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>130.184632</td>\n",
       "      <td>0.139012</td>\n",
       "      <td>211</td>\n",
       "      <td>311</td>\n",
       "      <td>260.369264</td>\n",
       "      <td>261.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>0.056847</td>\n",
       "      <td>5.890843</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>1843</td>\n",
       "      <td>3686</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332</th>\n",
       "      <td>3349</td>\n",
       "      <td>6698</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5333</th>\n",
       "      <td>3317</td>\n",
       "      <td>429198</td>\n",
       "      <td>0.007728</td>\n",
       "      <td>129.393428</td>\n",
       "      <td>0.078042</td>\n",
       "      <td>210</td>\n",
       "      <td>317</td>\n",
       "      <td>258.786856</td>\n",
       "      <td>259.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0.058965</td>\n",
       "      <td>5.952970</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334</th>\n",
       "      <td>4849</td>\n",
       "      <td>998689</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>205.957723</td>\n",
       "      <td>0.084966</td>\n",
       "      <td>342</td>\n",
       "      <td>494</td>\n",
       "      <td>411.915446</td>\n",
       "      <td>412.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>0.047067</td>\n",
       "      <td>6.300644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5335 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_nodes  num_edges  ratio_n_m   ratio_m_n   density  min_degree  \\\n",
       "0          3231       6094   0.530194    1.886103  0.001168           2   \n",
       "1          1129      53906   0.020944   47.746678  0.084657          65   \n",
       "2          4100    1254237   0.003269  305.911463  0.149262         538   \n",
       "3          4277      12831   0.333333    3.000000  0.001403           6   \n",
       "4          4712       9286   0.507431    1.970713  0.000837           2   \n",
       "...         ...        ...        ...         ...       ...         ...   \n",
       "5330       1874     243966   0.007681  130.184632  0.139012         211   \n",
       "5331       1843       3686   0.500000    2.000000  0.002172           4   \n",
       "5332       3349       6698   0.500000    2.000000  0.001195           4   \n",
       "5333       3317     429198   0.007728  129.393428  0.078042         210   \n",
       "5334       4849     998689   0.004855  205.957723  0.084966         342   \n",
       "\n",
       "      max_degree  mean_degree  median_degree  q0.25_degree  q0.75_degree  \\\n",
       "0              4     3.772207            4.0           4.0           4.0   \n",
       "1            122    95.493357           96.0          89.0         102.0   \n",
       "2            694   611.822927          611.0         597.0         627.0   \n",
       "3              6     6.000000            6.0           6.0           6.0   \n",
       "4              4     3.941426            4.0           4.0           4.0   \n",
       "...          ...          ...            ...           ...           ...   \n",
       "5330         311   260.369264          261.0         250.0         271.0   \n",
       "5331           4     4.000000            4.0           4.0           4.0   \n",
       "5332           4     4.000000            4.0           4.0           4.0   \n",
       "5333         317   258.786856          259.0         248.0         269.0   \n",
       "5334         494   411.915446          412.0         399.0         425.0   \n",
       "\n",
       "      variation_coefficient_degree  entropy_degree  best_alg_encoded  \n",
       "0                         0.111964        0.783043                 0  \n",
       "1                         0.096467        5.217304                 2  \n",
       "2                         0.036566        6.499606                 2  \n",
       "3                         0.000000       -0.000000                 0  \n",
       "4                         0.060489        0.324730                 2  \n",
       "...                            ...             ...               ...  \n",
       "5330                      0.056847        5.890843                 0  \n",
       "5331                      0.000000       -0.000000                 0  \n",
       "5332                      0.000000       -0.000000                 0  \n",
       "5333                      0.058965        5.952970                 0  \n",
       "5334                      0.047067        6.300644                 0  \n",
       "\n",
       "[5335 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] USING CPU\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(\"[i] USING CUDA\")\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "        print(\"[i] USING CPU\")\n",
    "    return device\n",
    "\n",
    "device = get_device() #setting up the DL device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pickle  # Import pickle for saving the scaler\n",
    "\n",
    "# Assuming 'your_dataframe' contains the DataFrame with the features and target column\n",
    "columns_to_normalize = [\n",
    "    'num_nodes', 'num_edges', 'ratio_n_m', 'ratio_m_n', 'density', 'min_degree', 'max_degree', 'mean_degree', 'median_degree',\n",
    "    'q0.25_degree', 'q0.75_degree', 'variation_coefficient_degree', 'entropy_degree'\n",
    "]\n",
    "\n",
    "# Exclude 'starting_node' and 'target_node' from columns_to_normalize\n",
    "columns_to_normalize = [col for col in columns_to_normalize]\n",
    "\n",
    "# Extract features and target columns\n",
    "features = df_train.drop(columns=[\"best_alg_encoded\"]).copy()\n",
    "target = df_train['best_alg_encoded'].copy()\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "features[columns_to_normalize] = scaler.fit_transform(features[columns_to_normalize])\n",
    "\n",
    "# Save the scaler to a file\n",
    "scaler_filename = \"./data/data_scaler.pkl\"\n",
    "with open(scaler_filename, 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding to the target variable\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Set sparse_output to False\n",
    "target_encoded = encoder.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "features_tensor = torch.from_numpy(features.values).float()\n",
    "target_tensor = torch.from_numpy(target_encoded).float()  # Use float instead of long for one-hot encoded targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>ratio_n_m</th>\n",
       "      <th>ratio_m_n</th>\n",
       "      <th>density</th>\n",
       "      <th>min_degree</th>\n",
       "      <th>max_degree</th>\n",
       "      <th>mean_degree</th>\n",
       "      <th>median_degree</th>\n",
       "      <th>q0.25_degree</th>\n",
       "      <th>q0.75_degree</th>\n",
       "      <th>variation_coefficient_degree</th>\n",
       "      <th>entropy_degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.626325</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.019332</td>\n",
       "      <td>0.114150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.205841</td>\n",
       "      <td>0.021106</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.094712</td>\n",
       "      <td>0.423149</td>\n",
       "      <td>0.071350</td>\n",
       "      <td>0.108303</td>\n",
       "      <td>0.094712</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.089990</td>\n",
       "      <td>0.099318</td>\n",
       "      <td>0.016657</td>\n",
       "      <td>0.760567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.800160</td>\n",
       "      <td>0.491166</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.606999</td>\n",
       "      <td>0.746249</td>\n",
       "      <td>0.590560</td>\n",
       "      <td>0.624549</td>\n",
       "      <td>0.606999</td>\n",
       "      <td>0.606151</td>\n",
       "      <td>0.603640</td>\n",
       "      <td>0.610516</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.947498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.835567</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>0.005919</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>0.006586</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.005919</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.006067</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.922585</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.003877</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.010444</td>\n",
       "      <td>0.047338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5330</th>\n",
       "      <td>0.354871</td>\n",
       "      <td>0.095535</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.258297</td>\n",
       "      <td>0.694989</td>\n",
       "      <td>0.231614</td>\n",
       "      <td>0.278881</td>\n",
       "      <td>0.258297</td>\n",
       "      <td>0.258929</td>\n",
       "      <td>0.252781</td>\n",
       "      <td>0.263875</td>\n",
       "      <td>0.009816</td>\n",
       "      <td>0.858753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>0.348670</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.008520</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332</th>\n",
       "      <td>0.649930</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.008520</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.005735</td>\n",
       "      <td>0.004391</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.003968</td>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5333</th>\n",
       "      <td>0.643529</td>\n",
       "      <td>0.168074</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.256727</td>\n",
       "      <td>0.390064</td>\n",
       "      <td>0.230516</td>\n",
       "      <td>0.284296</td>\n",
       "      <td>0.256727</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.250758</td>\n",
       "      <td>0.261928</td>\n",
       "      <td>0.010181</td>\n",
       "      <td>0.867810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334</th>\n",
       "      <td>0.949990</td>\n",
       "      <td>0.391091</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.408657</td>\n",
       "      <td>0.424694</td>\n",
       "      <td>0.375412</td>\n",
       "      <td>0.444043</td>\n",
       "      <td>0.408657</td>\n",
       "      <td>0.408730</td>\n",
       "      <td>0.403438</td>\n",
       "      <td>0.413827</td>\n",
       "      <td>0.008127</td>\n",
       "      <td>0.918493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5335 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_nodes  num_edges  ratio_n_m  ratio_m_n   density  min_degree  \\\n",
       "0      0.626325   0.002382   0.009037   0.003709  0.005600    0.002195   \n",
       "1      0.205841   0.021106   0.000324   0.094712  0.423149    0.071350   \n",
       "2      0.800160   0.491166   0.000022   0.606999  0.746249    0.590560   \n",
       "3      0.835567   0.005020   0.005669   0.005919  0.006777    0.006586   \n",
       "4      0.922585   0.003632   0.008647   0.003877  0.003944    0.002195   \n",
       "...         ...        ...        ...        ...       ...         ...   \n",
       "5330   0.354871   0.095535   0.000097   0.258297  0.694989    0.231614   \n",
       "5331   0.348670   0.001439   0.008520   0.003935  0.010620    0.004391   \n",
       "5332   0.649930   0.002619   0.008520   0.003935  0.005735    0.004391   \n",
       "5333   0.643529   0.168074   0.000098   0.256727  0.390064    0.230516   \n",
       "5334   0.949990   0.391091   0.000049   0.408657  0.424694    0.375412   \n",
       "\n",
       "      max_degree  mean_degree  median_degree  q0.25_degree  q0.75_degree  \\\n",
       "0       0.001805     0.003709       0.003968      0.004044      0.003895   \n",
       "1       0.108303     0.094712       0.095238      0.089990      0.099318   \n",
       "2       0.624549     0.606999       0.606151      0.603640      0.610516   \n",
       "3       0.003610     0.005919       0.005952      0.006067      0.005842   \n",
       "4       0.001805     0.003877       0.003968      0.004044      0.003895   \n",
       "...          ...          ...            ...           ...           ...   \n",
       "5330    0.278881     0.258297       0.258929      0.252781      0.263875   \n",
       "5331    0.001805     0.003935       0.003968      0.004044      0.003895   \n",
       "5332    0.001805     0.003935       0.003968      0.004044      0.003895   \n",
       "5333    0.284296     0.256727       0.256944      0.250758      0.261928   \n",
       "5334    0.444043     0.408657       0.408730      0.403438      0.413827   \n",
       "\n",
       "      variation_coefficient_degree  entropy_degree  \n",
       "0                         0.019332        0.114150  \n",
       "1                         0.016657        0.760567  \n",
       "2                         0.006314        0.947498  \n",
       "3                         0.000000        0.000000  \n",
       "4                         0.010444        0.047338  \n",
       "...                            ...             ...  \n",
       "5330                      0.009816        0.858753  \n",
       "5331                      0.000000        0.000000  \n",
       "5332                      0.000000        0.000000  \n",
       "5333                      0.010181        0.867810  \n",
       "5334                      0.008127        0.918493  \n",
       "\n",
       "[5335 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(5335, 13)\n",
      "--------------\n",
      "<class 'numpy.ndarray'>\n",
      "(5335, 3)\n"
     ]
    }
   ],
   "source": [
    "print(type(features))\n",
    "print(features.shape)\n",
    "print(\"--------------\")\n",
    "print(type(target_encoded))\n",
    "print(target_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the number or epochs and the learning rate\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "mini_batch_size = 16\n",
    "num_classes = 3\n",
    "\n",
    "input_size = features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3235  702 1398]\n",
      "[0.00030912 0.0014245  0.00071531]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "# Calculate class weights\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "print(class_sample_count)\n",
    "\n",
    "weight = 1. / class_sample_count\n",
    "print(weight)\n",
    "\n",
    "samples_weight = np.array([weight[t] for t in target])\n",
    "\n",
    "# Create a WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# Assuming 'features_tensor' and 'target_tensor' are already created\n",
    "\n",
    "# Combine features and target into a TensorDataset\n",
    "dataset = TensorDataset(features_tensor, target_tensor)\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Class Counts: [1752, 1845, 1738]\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for each class\n",
    "class_counts = [0] * num_classes  # Replace num_classes with the actual number of classes in your dataset\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch_idx, (x, y_one_hot) in enumerate(dataloader):\n",
    "    # Convert one-hot encoded labels to class indices\n",
    "    y = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "    # Update class counts for each batch\n",
    "    for class_idx in range(num_classes):\n",
    "        class_counts[class_idx] += len(torch.where(y == class_idx)[0])\n",
    "\n",
    "    # Print class distribution within each batch\n",
    "    batch_class_counts = [len(torch.where(y == class_idx)[0]) for class_idx in range(num_classes)]\n",
    "    #print(f\"Batch {batch_idx} Class Distribution: {batch_class_counts}\")\n",
    "\n",
    "# Print cumulative count of elements in different classes\n",
    "print(\"Cumulative Class Counts:\", class_counts)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# Assuming 'dataset' is the TensorDataset you created earlier\n",
    "\n",
    "# Calculate the sizes for training and validation sets\n",
    "total_samples = len(dataset)\n",
    "train_size = int(0.9 * total_samples)\n",
    "val_size = total_samples - train_size\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_set, validation_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training set\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=True, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "# define training function\n",
    "\n",
    "# implement early stopping for training function\n",
    "# from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss <= self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"[i] Validation Loss Increased - Early Stop!\")\n",
    "                print(\n",
    "                    f\"--- {validation_loss} > {self.min_validation_loss + self.min_delta} ---\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def train(net, train_loader, validation_loader, num_epochs, batch_size, mini_batch_size, optimizer, lr_scheduler, criterion, earlystop_patience=0, earlystop_min_delta=1e-6, name=\"\"):\n",
    "    # Save the loss into a dataframe\n",
    "    losses = pd.DataFrame(index=list(range(num_epochs)), columns=[\n",
    "                          'running_loss', 'train_loss', 'valid_loss'])\n",
    "    min_validation_loss = np.inf\n",
    "\n",
    "    # Use a summary writer to check loss in real time\n",
    "    current_time = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    writer = SummaryWriter(\n",
    "        f'runs/tensorboard/{current_time}_{(net.__class__.__name__).lower()}_{name}')\n",
    "\n",
    "    # Set early stopping parameters\n",
    "    # from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "    early_stopping = EarlyStopper(\n",
    "        patience=earlystop_patience, min_delta=earlystop_min_delta)\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "\n",
    "    net.train()\n",
    "    net.to(device)  # Move the model to the specified device\n",
    "\n",
    "    for epoch in range(num_epochs):  # Looping over the dataset\n",
    "\n",
    "        running_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        net.train()  # Set the model to training mode\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            start_time_mini_batch = time.time()\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Setting the parameter gradients to zero\n",
    "            outputs = net(inputs)  # Forward pass\n",
    "\n",
    "            labels = labels.float()\n",
    "\n",
    "            loss = criterion(outputs, labels)  # Applying the criterion\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimization step\n",
    "\n",
    "            running_loss += loss.item()  # Updating the running loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if i % mini_batch_size == mini_batch_size - 1:  # Printing the running loss\n",
    "                print(f\"[epoch: {epoch + 1}, mini-batch: {i + 1}, time-taken: {round(time.time() - start_time_mini_batch, 3)} sec] loss: {round(running_loss / mini_batch_size, 6)} \")\n",
    "\n",
    "                # write on the summary writer\n",
    "                writer.add_scalar(\n",
    "                    'Loss/Running', running_loss / mini_batch_size, i)\n",
    "\n",
    "                running_loss = 0.0\n",
    "                start_time_mini_batch = time.time()\n",
    "\n",
    "\n",
    "        net.eval().to(device)\n",
    "\n",
    "        # Inside the validation loop\n",
    "        with torch.no_grad():\n",
    "            net.eval()  # Set the model to evaluation mode\n",
    "            all_labels = []\n",
    "            all_outputs = []\n",
    "\n",
    "            for i, data in enumerate(validation_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)  # Forward pass\n",
    "\n",
    "                loss = criterion(outputs, labels) # Applying the criterion\n",
    "                validation_loss += loss.item() # Check the loss\n",
    "\n",
    "                # Use argmax to get the index of the predicted class\n",
    "                predicted_class = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                # Append predictions and labels for accuracy calculation\n",
    "                all_outputs.extend(F.one_hot(predicted_class, num_classes=num_classes).cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "            # Convert lists to numpy arrays for easier computation\n",
    "            all_labels = np.array(all_labels)\n",
    "            all_outputs = np.array(all_outputs)\n",
    "            \n",
    "            print(all_outputs, all_labels)\n",
    "\n",
    "            val_accuracy = accuracy_score(all_labels, all_outputs)\n",
    "            val_precision = precision_score(all_labels, all_outputs, average='weighted', zero_division=1.0)\n",
    "            val_recall = recall_score(all_labels, all_outputs, average='weighted', zero_division=1.0)\n",
    "            val_f1 = f1_score(all_labels, all_outputs, average='weighted', zero_division=1.0)\n",
    "\n",
    "            # Print or log the accuracy and validation loss\n",
    "            print(f'+++ [\\033[1mepoch: {epoch + 1}\\033[0m, validation - \\033[91maccuracy: {val_accuracy:.5f}\\033[0m, \\033[93mprecision: {val_precision:.5f}\\033[0m, \\033[94mrecall: {val_recall:.5f}\\033[0m, \\033[95mf1-score: {val_f1:.5f}\\033[0m] +++')\n",
    "\n",
    "        # Switch back to training mode for the next epoch\n",
    "        net.train().to(device)\n",
    "\n",
    "        print('+++ [epoch: %d, training loss: %.5f, validation loss: %.5f] +++' %\n",
    "              (epoch + 1,\n",
    "               train_loss / len(train_loader),\n",
    "               validation_loss / len(validation_loader)))\n",
    "\n",
    "        print(\n",
    "            f\"--- time-taken for epoch {epoch+1}: {round(time.time() - start_time_epoch, 3)} seconds ---\")\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        # Saving the loss\n",
    "        losses.at[epoch, 'running_loss'] = running_loss\n",
    "        losses.at[epoch, 'train_loss'] = train_loss\n",
    "        losses.at[epoch, 'valid_loss'] = validation_loss\n",
    "\n",
    "        # Write on the summary writer\n",
    "        writer.add_scalar('Loss/Train', train_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar('Loss/Validation', validation_loss /\n",
    "                          len(validation_loader), epoch)\n",
    "\n",
    "        # Update the learning rate\n",
    "        if lr_scheduler.__class__.__name__ == \"CosineAnnealingWarmRestarts\" and lr_scheduler is not None:\n",
    "            print(f\"\\033[90m--- current LR: {round(lr_scheduler.get_last_lr()[0], 9)} ---\\033[0m\")\n",
    "            lr_scheduler.step()  # step scheduler learning rate\n",
    "\n",
    "        if min_validation_loss > (validation_loss / len(validation_loader)):\n",
    "            print(f'\\033[92m+++ [validation loss decreased ({min_validation_loss:.9f} -> {(validation_loss / len(validation_loader)):.9f}), saving the model ...] +++\\033[0m')\n",
    "            min_validation_loss = validation_loss / len(validation_loader)\n",
    "\n",
    "            # Check if the directory exists, and if not, create it\n",
    "            save_dir = f'./runs/models/{(net.__class__.__name__).lower()}'\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            # Save State Dict\n",
    "            torch.save(net.state_dict(), f'{save_dir}/{(net.__class__.__name__).lower()}_{name}_saved_model.pth')\n",
    "\n",
    "        # Check if early stopping criteria is fulfilled\n",
    "        if early_stopping.early_stop(validation_loss):\n",
    "            break\n",
    "\n",
    "    pickle.dump(losses, open(\n",
    "        f'./runs/models/{(net.__class__.__name__).lower()}/{(net.__class__.__name__).lower()}_{name}_loss.pkl', 'wb'))\n",
    "    writer.close()\n",
    "    print(f\"[i] Finished Training\")\n",
    "\n",
    "    # Create DataLoader for training set\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    # Create DataLoader for validation set\n",
    "    validation_loader = DataLoader(\n",
    "        validation_set, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]           3,584\n",
      "           Dropout-2                  [-1, 256]               0\n",
      "            Linear-3                  [-1, 128]          32,896\n",
      "           Dropout-4                  [-1, 128]               0\n",
      "            Linear-5                   [-1, 64]           8,256\n",
      "           Dropout-6                   [-1, 64]               0\n",
      "            Linear-7                   [-1, 32]           2,080\n",
      "           Dropout-8                   [-1, 32]               0\n",
      "            Linear-9                   [-1, 16]             528\n",
      "          Dropout-10                   [-1, 16]               0\n",
      "           Linear-11                    [-1, 8]             136\n",
      "          Dropout-12                    [-1, 8]               0\n",
      "           Linear-13                    [-1, 3]              27\n",
      "================================================================\n",
      "Total params: 47,507\n",
      "Trainable params: 47,507\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.18\n",
      "Estimated Total Size (MB): 0.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleLinearNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleLinearNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.fc5 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.fc6 = nn.Linear(in_features=16, out_features=8)\n",
    "        self.fc7 = nn.Linear(in_features=8, out_features=num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.tanh(self.fc1(x)))\n",
    "        x = self.dropout(F.tanh(self.fc2(x)))\n",
    "        x = self.dropout(F.tanh(self.fc3(x)))\n",
    "        x = self.dropout(F.elu(self.fc4(x)))\n",
    "        x = self.dropout(F.elu(self.fc5(x)))\n",
    "        x = self.dropout(F.elu(self.fc6(x)))\n",
    "        x = F.softmax(self.fc7(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Create and print the model\n",
    "model = SimpleLinearNet(input_size=input_size, num_classes=num_classes).to(device)\n",
    "summary(model, (input_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Traing the network SimpleLinearNet ...\n",
      "[epoch: 1, mini-batch: 16, time-taken: 0.008 sec] loss: 0.988147 \n",
      "[epoch: 1, mini-batch: 32, time-taken: 0.004 sec] loss: 0.953411 \n",
      "[epoch: 1, mini-batch: 48, time-taken: 0.007 sec] loss: 0.939653 \n",
      "[epoch: 1, mini-batch: 64, time-taken: 0.008 sec] loss: 0.936545 \n",
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]] [[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "+++ [\u001b[1mepoch: 1\u001b[0m, validation - \u001b[91maccuracy: 0.61236\u001b[0m, \u001b[93mprecision: 0.76262\u001b[0m, \u001b[94mrecall: 0.61236\u001b[0m, \u001b[95mf1-score: 0.46514\u001b[0m] +++\n",
      "+++ [epoch: 1, training loss: 0.96656, validation loss: 0.94394] +++\n",
      "--- time-taken for epoch 1: 17.028 seconds ---\n",
      "\u001b[90m--- current LR: 0.003 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (inf -> 0.943937348), saving the model ...] +++\u001b[0m\n",
      "[epoch: 2, mini-batch: 16, time-taken: 0.003 sec] loss: 0.976523 \n",
      "[epoch: 2, mini-batch: 32, time-taken: 0.005 sec] loss: 0.946392 \n",
      "[epoch: 2, mini-batch: 48, time-taken: 0.006 sec] loss: 0.924533 \n",
      "[epoch: 2, mini-batch: 64, time-taken: 0.005 sec] loss: 0.938655 \n",
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]] [[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "+++ [\u001b[1mepoch: 2\u001b[0m, validation - \u001b[91maccuracy: 0.61236\u001b[0m, \u001b[93mprecision: 0.76262\u001b[0m, \u001b[94mrecall: 0.61236\u001b[0m, \u001b[95mf1-score: 0.46514\u001b[0m] +++\n",
      "+++ [epoch: 2, training loss: 0.94186, validation loss: 0.94395] +++\n",
      "--- time-taken for epoch 2: 7.088 seconds ---\n",
      "\u001b[90m--- current LR: 0.002799038 ---\u001b[0m\n",
      "[i] Validation Loss Increased - Early Stop!\n",
      "--- 8.495554625988007 > 8.495437131954192 ---\n",
      "[i] Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = SimpleLinearNet(input_size=input_size,num_classes=num_classes).to(device)\n",
    "train_flag = True # Dont run if False\n",
    "name = \"simple_net\"\n",
    "\n",
    "if train_flag:\n",
    "    print(f\"[i] Traing the network {net.__class__.__name__} ...\")\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate = 3e-3\n",
    "    \n",
    "    # Define your criterion (e.g., CrossEntropyLoss for multiclass classification)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Define your optimizer (e.g., Adam)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Cosine Annealing with Restarts (CWR) scheduler.\n",
    "    # This scheduler is designed to automatically adjust the learning rate according to a cosine wave, and can be used to adjust the learning rate as the model converges.\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=round((1/10)*batch_size), T_mult=1, eta_min=0)\n",
    "\n",
    "    # Train the model\n",
    "    train(net, train_loader, validation_loader, num_epochs, batch_size, mini_batch_size, optimizer, scheduler, criterion, name=name)\n",
    "else: # load the model\n",
    "    print(f\"[i] Loading the network {net.__class__.__name__} ...\")\n",
    "    #Loading existing models (with saved weights)\n",
    "    net.load_state_dict(torch.load(f'./runs/models/{(net.__class__.__name__).lower()}/{(net.__class__.__name__).lower()}_{name}_saved_model.pth', map_location=device)) #using saved data if present\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]           3,584\n",
      "       BatchNorm1d-2                  [-1, 256]             512\n",
      "           Dropout-3                  [-1, 256]               0\n",
      "            Linear-4                  [-1, 128]          32,896\n",
      "       BatchNorm1d-5                  [-1, 128]             256\n",
      "           Dropout-6                  [-1, 128]               0\n",
      "            Linear-7                   [-1, 64]           8,256\n",
      "       BatchNorm1d-8                   [-1, 64]             128\n",
      "           Dropout-9                   [-1, 64]               0\n",
      "           Linear-10                   [-1, 32]           2,080\n",
      "      BatchNorm1d-11                   [-1, 32]              64\n",
      "          Dropout-12                   [-1, 32]               0\n",
      "           Linear-13                   [-1, 16]             528\n",
      "      BatchNorm1d-14                   [-1, 16]              32\n",
      "          Dropout-15                   [-1, 16]               0\n",
      "           Linear-16                    [-1, 8]             136\n",
      "      BatchNorm1d-17                    [-1, 8]              16\n",
      "          Dropout-18                    [-1, 8]               0\n",
      "           Linear-19                    [-1, 3]              27\n",
      "================================================================\n",
      "Total params: 48,515\n",
      "Trainable params: 48,515\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 0.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class MoreComplexNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, init_fn):\n",
    "        super(MoreComplexNet, self).__init__()\n",
    "        self.init_fn = init_fn\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.fc5 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.bn5 = nn.BatchNorm1d(16)\n",
    "\n",
    "        self.fc6 = nn.Linear(in_features=16, out_features=8)\n",
    "        self.bn6 = nn.BatchNorm1d(8)\n",
    "\n",
    "        self.fc7 = nn.Linear(in_features=8, out_features=num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            self.init_fn(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.tanh(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout(F.elu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout(F.elu(self.bn3(self.fc3(x))))\n",
    "        x = self.dropout(F.elu(self.bn4(self.fc4(x))))\n",
    "        x = self.dropout(F.elu(self.bn5(self.fc5(x))))\n",
    "        x = self.dropout(F.elu(self.bn6(self.fc6(x))))\n",
    "        x = F.softmax(self.fc7(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "# Create and print the model\n",
    "model = MoreComplexNet(input_size, num_classes=num_classes, init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "summary(model, (input_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Traing the network MoreComplexNet ...\n",
      "[epoch: 1, mini-batch: 16, time-taken: 0.009 sec] loss: 1.059618 \n",
      "[epoch: 1, mini-batch: 32, time-taken: 0.007 sec] loss: 1.009864 \n",
      "[epoch: 1, mini-batch: 48, time-taken: 0.007 sec] loss: 0.984653 \n",
      "[epoch: 1, mini-batch: 64, time-taken: 0.009 sec] loss: 0.98534 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 256])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m     scheduler \u001b[38;5;241m=\u001b[39m CosineAnnealingWarmRestarts(\n\u001b[0;32m     21\u001b[0m         optimizer, T_0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m*\u001b[39mbatch_size), T_mult\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmini_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# load the model\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[i] Loading the network \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnet\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 68\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_loader, validation_loader, num_epochs, batch_size, mini_batch_size, optimizer, lr_scheduler, criterion, earlystop_patience, earlystop_min_delta, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Setting the parameter gradients to zero\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     70\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     72\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Applying the criterion\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[21], line 36\u001b[0m, in \u001b[0;36mMoreComplexNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(F\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))))\n\u001b[0;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2436\u001b[0m         batch_norm,\n\u001b[0;32m   2437\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2445\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2446\u001b[0m     )\n\u001b[0;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m-> 2448\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m   2451\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[0;32m   2452\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[1;34m(size)\u001b[0m\n\u001b[0;32m   2414\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   2415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 2416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(size))\n",
      "\u001b[1;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 256])"
     ]
    }
   ],
   "source": [
    "net = MoreComplexNet(input_dim=input_size, num_classes=num_classes,\n",
    "                     init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "train_flag = True\n",
    "name = \"more_complex_net\"\n",
    "\n",
    "if train_flag:\n",
    "    print(f\"[i] Traing the network {net.__class__.__name__} ...\")\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate = 5e-3\n",
    "\n",
    "    # Define your criterion (e.g., CrossEntropyLoss for multiclass classification)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Define your optimizer (e.g., Adam)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Cosine Annealing with Restarts (CWR) scheduler.\n",
    "    # This scheduler is designed to automatically adjust the learning rate according to a cosine wave, and can be used to adjust the learning rate as the model converges.\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=round((1/10)*batch_size), T_mult=1, eta_min=0)\n",
    "\n",
    "    # Train the model\n",
    "    train(net, train_loader, validation_loader, num_epochs, batch_size,\n",
    "          mini_batch_size, optimizer, scheduler, criterion, name=name)\n",
    "else:  # load the model\n",
    "    print(f\"[i] Loading the network {net.__class__.__name__} ...\")\n",
    "    #Loading existing models (with saved weights)\n",
    "    net.load_state_dict(torch.load(f'./runs/models/{(net.__class__.__name__).lower()}/{(net.__class__.__name__).lower()}_{name}_saved_model.pth', map_location=device)) #using saved data if present\n",
    "    net.eval()\n",
    "\n",
    "# Best loss found 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1024]          14,336\n",
      "       BatchNorm1d-2                 [-1, 1024]           2,048\n",
      "           Dropout-3                 [-1, 1024]               0\n",
      "            Linear-4                  [-1, 512]         524,800\n",
      "       BatchNorm1d-5                  [-1, 512]           1,024\n",
      "           Dropout-6                  [-1, 512]               0\n",
      "            Linear-7                  [-1, 256]         131,328\n",
      "       BatchNorm1d-8                  [-1, 256]             512\n",
      "           Dropout-9                  [-1, 256]               0\n",
      "           Linear-10                  [-1, 128]          32,896\n",
      "      BatchNorm1d-11                  [-1, 128]             256\n",
      "          Dropout-12                  [-1, 128]               0\n",
      "           Linear-13                   [-1, 64]           8,256\n",
      "      BatchNorm1d-14                   [-1, 64]             128\n",
      "          Dropout-15                   [-1, 64]               0\n",
      "           Linear-16                   [-1, 32]           2,080\n",
      "      BatchNorm1d-17                   [-1, 32]              64\n",
      "          Dropout-18                   [-1, 32]               0\n",
      "           Linear-19                   [-1, 16]             528\n",
      "      BatchNorm1d-20                   [-1, 16]              32\n",
      "          Dropout-21                   [-1, 16]               0\n",
      "           Linear-22                    [-1, 8]             136\n",
      "      BatchNorm1d-23                    [-1, 8]              16\n",
      "          Dropout-24                    [-1, 8]               0\n",
      "           Linear-25                    [-1, 3]              27\n",
      "================================================================\n",
      "Total params: 718,467\n",
      "Trainable params: 718,467\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 2.74\n",
      "Estimated Total Size (MB): 2.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class EvenMoreComplexNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, init_fn):\n",
    "        super(EvenMoreComplexNet, self).__init__()\n",
    "        self.init_fn = init_fn\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc4 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc5 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.fc6 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.bn6 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.fc7 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.bn7 = nn.BatchNorm1d(16)\n",
    "\n",
    "        self.fc8 = nn.Linear(in_features=16, out_features=8)\n",
    "        self.bn8 = nn.BatchNorm1d(8)\n",
    "\n",
    "        self.fc_out = nn.Linear(in_features=8, out_features=num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            self.init_fn(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.tanh(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout(F.elu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout(F.elu(self.bn3(self.fc3(x))))\n",
    "        x = self.dropout(F.elu(self.bn4(self.fc4(x))))\n",
    "        x = self.dropout(F.elu(self.bn5(self.fc5(x))))\n",
    "        x = self.dropout(F.elu(self.bn6(self.fc6(x))))\n",
    "        x = self.dropout(F.elu(self.bn7(self.fc7(x))))\n",
    "        x = self.dropout(F.elu(self.bn8(self.fc8(x))))\n",
    "        x = F.softmax(self.fc_out(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Create and print the model\n",
    "model = EvenMoreComplexNet(input_size, num_classes=num_classes, init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "summary(model, (input_size,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Traing the network EvenMoreComplexNet ...\n",
      "[epoch: 1, mini-batch: 16, time-taken: 0.016 sec] loss: 1.079372 \n",
      "[epoch: 1, mini-batch: 32, time-taken: 0.02 sec] loss: 1.031443 \n",
      "[epoch: 1, mini-batch: 48, time-taken: 0.017 sec] loss: 1.004605 \n",
      "[epoch: 1, mini-batch: 64, time-taken: 0.016 sec] loss: 0.96853 \n",
      "+++ [\u001b[1mepoch: 1\u001b[0m, validation - \u001b[91maccuracy: 0.71681\u001b[0m, \u001b[93mprecision: 0.57978\u001b[0m, \u001b[94mrecall: 0.54661\u001b[0m, \u001b[95mf1-score: 0.56270\u001b[0m] +++\n",
      "+++ [epoch: 1, training loss: 1.01957, validation loss: 0.97003] +++\n",
      "--- time-taken for epoch 1: 7.625 seconds ---\n",
      "\u001b[90m--- current LR: 0.005 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (inf -> 0.970033355), saving the model ...] +++\u001b[0m\n",
      "[epoch: 2, mini-batch: 16, time-taken: 0.017 sec] loss: 0.982261 \n",
      "[epoch: 2, mini-batch: 32, time-taken: 0.023 sec] loss: 0.968431 \n",
      "[epoch: 2, mini-batch: 48, time-taken: 0.021 sec] loss: 0.935523 \n",
      "[epoch: 2, mini-batch: 64, time-taken: 0.013 sec] loss: 0.970363 \n",
      "+++ [\u001b[1mepoch: 2\u001b[0m, validation - \u001b[91maccuracy: 0.71822\u001b[0m, \u001b[93mprecision: 0.58093\u001b[0m, \u001b[94mrecall: 0.55508\u001b[0m, \u001b[95mf1-score: 0.56771\u001b[0m] +++\n",
      "+++ [epoch: 2, training loss: 0.96220, validation loss: 0.97321] +++\n",
      "--- time-taken for epoch 2: 7.936 seconds ---\n",
      "\u001b[90m--- current LR: 0.004665064 ---\u001b[0m\n",
      "[i] Validation Loss Increased - Early Stop!\n",
      "--- 7.7857131361961365 > 7.760267840457916 ---\n",
      "[i] Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = EvenMoreComplexNet(input_dim=input_size, num_classes=num_classes,\n",
    "                     init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "train_flag = True\n",
    "name = \"even_more_complex_net\"\n",
    "\n",
    "if train_flag:\n",
    "    print(f\"[i] Traing the network {net.__class__.__name__} ...\")\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate = 5e-3\n",
    "\n",
    "    # Define your criterion (e.g., CrossEntropyLoss for multiclass classification)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Define your optimizer (e.g., Adam)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Cosine Annealing with Restarts (CWR) scheduler.\n",
    "    # This scheduler is designed to automatically adjust the learning rate according to a cosine wave, and can be used to adjust the learning rate as the model converges.\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=round((1/10)*batch_size), T_mult=1, eta_min=0)\n",
    "\n",
    "    # Train the model\n",
    "    train(net, train_loader, validation_loader, num_epochs, batch_size,\n",
    "          mini_batch_size, optimizer, scheduler, criterion, name=name)\n",
    "else:  # load the model\n",
    "    print(f\"[i] Loading the network {net.__class__.__name__} ...\")\n",
    "    #Loading existing models (with saved weights)\n",
    "    net.load_state_dict(torch.load(f'./runs/models/{(net.__class__.__name__).lower()}/{(net.__class__.__name__).lower()}_{name}_saved_model.pth', map_location=device)) #using saved data if present\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 512]           7,168\n",
      "           Dropout-2                  [-1, 512]               0\n",
      "            Linear-3                  [-1, 512]         262,656\n",
      "           Dropout-4                  [-1, 512]               0\n",
      "            Linear-5                    [-1, 3]           1,539\n",
      "================================================================\n",
      "Total params: 271,363\n",
      "Trainable params: 271,363\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 1.04\n",
      "Estimated Total Size (MB): 1.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SimplestNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, init_fn):\n",
    "        super(SimplestNet, self).__init__()\n",
    "\n",
    "        self.init_fn = init_fn\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            self.init_fn(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# Create and print the model\n",
    "model = SimplestNet(input_size=input_size, hidden_size=512, num_classes=num_classes, init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "summary(model, (input_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Traing the network SimplestNet ...\n",
      "[epoch: 1, mini-batch: 16, time-taken: 0.01 sec] loss: 1.087399 \n",
      "[epoch: 1, mini-batch: 32, time-taken: 0.008 sec] loss: 1.061582 \n",
      "[epoch: 1, mini-batch: 48, time-taken: 0.009 sec] loss: 1.034582 \n",
      "[epoch: 1, mini-batch: 64, time-taken: 0.006 sec] loss: 0.997247 \n",
      "+++ [\u001b[1mepoch: 1\u001b[0m, validation - \u001b[91maccuracy: 0.75000\u001b[0m, \u001b[93mprecision: 0.62500\u001b[0m, \u001b[94mrecall: 0.62500\u001b[0m, \u001b[95mf1-score: 0.62500\u001b[0m] +++\n",
      "+++ [epoch: 1, training loss: 1.04151, validation loss: 0.96434] +++\n",
      "--- time-taken for epoch 1: 6.85 seconds ---\n",
      "\u001b[90m--- current LR: 0.001 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (inf -> 0.964342922), saving the model ...] +++\u001b[0m\n",
      "[epoch: 2, mini-batch: 16, time-taken: 0.01 sec] loss: 0.95369 \n",
      "[epoch: 2, mini-batch: 32, time-taken: 0.008 sec] loss: 0.944112 \n",
      "[epoch: 2, mini-batch: 48, time-taken: 0.009 sec] loss: 0.942537 \n",
      "[epoch: 2, mini-batch: 64, time-taken: 0.009 sec] loss: 0.951701 \n",
      "+++ [\u001b[1mepoch: 2\u001b[0m, validation - \u001b[91maccuracy: 0.75000\u001b[0m, \u001b[93mprecision: 0.62500\u001b[0m, \u001b[94mrecall: 0.62500\u001b[0m, \u001b[95mf1-score: 0.62500\u001b[0m] +++\n",
      "+++ [epoch: 2, training loss: 0.94818, validation loss: 0.92451] +++\n",
      "--- time-taken for epoch 2: 6.986 seconds ---\n",
      "\u001b[90m--- current LR: 0.000933013 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (0.964342922 -> 0.924505040), saving the model ...] +++\u001b[0m\n",
      "[epoch: 3, mini-batch: 16, time-taken: 0.007 sec] loss: 0.938857 \n",
      "[epoch: 3, mini-batch: 32, time-taken: 0.006 sec] loss: 0.925617 \n",
      "[epoch: 3, mini-batch: 48, time-taken: 0.009 sec] loss: 0.916387 \n",
      "[epoch: 3, mini-batch: 64, time-taken: 0.008 sec] loss: 0.945366 \n",
      "+++ [\u001b[1mepoch: 3\u001b[0m, validation - \u001b[91maccuracy: 0.74788\u001b[0m, \u001b[93mprecision: 0.62366\u001b[0m, \u001b[94mrecall: 0.61441\u001b[0m, \u001b[95mf1-score: 0.61900\u001b[0m] +++\n",
      "+++ [epoch: 3, training loss: 0.93179, validation loss: 0.92842] +++\n",
      "--- time-taken for epoch 3: 6.932 seconds ---\n",
      "\u001b[90m--- current LR: 0.00075 ---\u001b[0m\n",
      "[i] Validation Loss Increased - Early Stop!\n",
      "--- 7.427349805831909 > 7.3960413203964235 ---\n",
      "[i] Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = SimplestNet(input_size=input_size, hidden_size=512, num_classes=num_classes, init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "train_flag = True\n",
    "name = \"simplest_net\"\n",
    "\n",
    "if train_flag:\n",
    "    print(f\"[i] Traing the network {net.__class__.__name__} ...\")\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # Define your criterion (e.g., CrossEntropyLoss for multiclass classification)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Define your optimizer (e.g., Adam)\n",
    "    optimizer = optim.RAdam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Cosine Annealing with Restarts (CWR) scheduler.\n",
    "    # This scheduler is designed to automatically adjust the learning rate according to a cosine wave, and can be used to adjust the learning rate as the model converges.\n",
    "    scheduler = CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=round((1/10)*batch_size), T_mult=1, eta_min=0)\n",
    "\n",
    "    # Train the model\n",
    "    train(net, train_loader, validation_loader, num_epochs, batch_size,\n",
    "          mini_batch_size, optimizer, scheduler, criterion, name=name)\n",
    "else:  # load the model\n",
    "    print(f\"[i] Loading the network {net.__class__.__name__} ...\")\n",
    "    #Loading existing models (with saved weights)\n",
    "    net.load_state_dict(torch.load(f'./runs/models/{(net.__class__.__name__).lower()}/{(net.__class__.__name__).lower()}_{name}_saved_model.pth', map_location=device)) #using saved data if present\n",
    "    net.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
