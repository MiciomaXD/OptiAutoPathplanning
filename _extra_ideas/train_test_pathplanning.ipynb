{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, csv file I/O (e.g. pd.read_csv)\n",
    "import os # deal with os primitives\n",
    "import time # time-related functions\n",
    "import matplotlib.pyplot as plt # plots\n",
    "import pickle # object serialization\n",
    "from collections import Counter # dict subclass for counting hashable items\n",
    "from tqdm import tqdm # iterable object\n",
    "import random # random values generator\n",
    "import seaborn as sns # prettier plots\n",
    "import torch # main package for PyTorch\n",
    "import torch.utils.data as data_utils # access data sets, including pre-processing, loading, and splitting\n",
    "from torch.utils.data import random_split # randomly split a dataset \n",
    "import torch.optim as optim # optimization algorithms\n",
    "import torch.nn as nn # build neural network (layers, activations, loss functions)\n",
    "import torch.nn.functional as F # functions used to build neural network\n",
    "from torchsummary import summary # print the summary of a neural network model\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts # scheduler used to adjust the learning rate\n",
    "from torch.utils.tensorboard.writer import SummaryWriter # nn log writer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "torch.manual_seed(1111)\n",
    "torch.cuda.manual_seed(1111)\n",
    "np.random.seed(1111)\n",
    "random.seed(1111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train set\n",
    "df_train = pd.DataFrame(pd.read_pickle(\"df_train.pkl\"))\n",
    "\n",
    "# Load the test set\n",
    "df_test = pd.DataFrame(pd.read_pickle(\"df_test.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>graph</th>\n",
       "      <th>label_time</th>\n",
       "      <th>label_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)</td>\n",
       "      <td>bfs</td>\n",
       "      <td>dfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>dfs</td>\n",
       "      <td>dfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>best_first_search</td>\n",
       "      <td>astar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)</td>\n",
       "      <td>bfs</td>\n",
       "      <td>best_first_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>best_first_search</td>\n",
       "      <td>astar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               graph         label_time  \\\n",
       "0     (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)                bfs   \n",
       "1  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...                dfs   \n",
       "2  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  best_first_search   \n",
       "3             (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)                bfs   \n",
       "4  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  best_first_search   \n",
       "\n",
       "         label_space  \n",
       "0                dfs  \n",
       "1                dfs  \n",
       "2              astar  \n",
       "3  best_first_search  \n",
       "4              astar  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>graph</th>\n",
       "      <th>label_time</th>\n",
       "      <th>label_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)</td>\n",
       "      <td>bfs</td>\n",
       "      <td>dfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>dfs</td>\n",
       "      <td>dfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>best_first_search</td>\n",
       "      <td>astar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)</td>\n",
       "      <td>bfs</td>\n",
       "      <td>best_first_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>best_first_search</td>\n",
       "      <td>astar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>dfs</td>\n",
       "      <td>best_first_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>best_first_search</td>\n",
       "      <td>astar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>best_first_search</td>\n",
       "      <td>best_first_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>best_first_search</td>\n",
       "      <td>randomized_shortest_path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>best_first_search</td>\n",
       "      <td>best_first_search</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  graph         label_time  \\\n",
       "0        (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13)                bfs   \n",
       "1     (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...                dfs   \n",
       "2     (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  best_first_search   \n",
       "3                (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)                bfs   \n",
       "4     (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  best_first_search   \n",
       "...                                                 ...                ...   \n",
       "1795  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...                dfs   \n",
       "1796  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  best_first_search   \n",
       "1797  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  best_first_search   \n",
       "1798  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  best_first_search   \n",
       "1799  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  best_first_search   \n",
       "\n",
       "                   label_space  \n",
       "0                          dfs  \n",
       "1                          dfs  \n",
       "2                        astar  \n",
       "3            best_first_search  \n",
       "4                        astar  \n",
       "...                        ...  \n",
       "1795         best_first_search  \n",
       "1796                     astar  \n",
       "1797         best_first_search  \n",
       "1798  randomized_shortest_path  \n",
       "1799         best_first_search  \n",
       "\n",
       "[1800 rows x 3 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Features Available: {'coord2d'}\n",
      "Edge Features Available: {'weight'}\n"
     ]
    }
   ],
   "source": [
    "for _, row in df_train.iterrows():\n",
    "    graph = row[\"graph\"]\n",
    "    label = row[\"label_time\"]\n",
    "\n",
    "    # Check for node features\n",
    "    node_features_available = set()\n",
    "    for node in graph.nodes:\n",
    "        node_features_available.update(graph.nodes[node].keys())\n",
    "\n",
    "    print(\"Node Features Available:\", node_features_available)\n",
    "\n",
    "    # Check for edge features\n",
    "    edge_features_available = set()\n",
    "    for edge in graph.edges:\n",
    "        edge_features_available.update(graph.edges[edge].keys())\n",
    "\n",
    "    print(\"Edge Features Available:\", edge_features_available)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the number or epochs and the learning rate\n",
    "num_epochs = 1000\n",
    "batch_size = 256\n",
    "mini_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bfs', 'dfs', 'best_first_search', 'bidirectional_search',\n",
       "       'dijkstra', 'randomized_shortest_path', 'astar'], dtype=object)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"label_time\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert NetworkX graph to PyTorch Geometric Data\n",
    "def convert_nx_to_pyg_data(nx_graph, label):\n",
    "    # Extract node features from NetworkX graph\n",
    "    node_features = torch.tensor([list(nx_graph.nodes[node]['coord2d']) for node in nx_graph.nodes], dtype=torch.float32)\n",
    "\n",
    "    # Extract edge indices from NetworkX graph as pairs\n",
    "    edge_index = torch.tensor([[edge[0], edge[1]] for edge in nx_graph.edges], dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # If your graph has edge features, extract them similarly\n",
    "    edge_features = torch.tensor([nx_graph.edges[edge]['weight'] for edge in nx_graph.edges], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    # Create a PyTorch Geometric Data object\n",
    "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features, y=label)\n",
    "    return data\n",
    "\n",
    "# Convert each row in df_train to PyTorch Geometric Data\n",
    "label_mapping = {\n",
    "    'bfs': 0,\n",
    "    'dfs': 1,\n",
    "    'best_first_search': 2,\n",
    "    'bidirectional_search': 3,\n",
    "    'dijkstra': 4,\n",
    "    'randomized_shortest_path': 5,\n",
    "    'astar': 6\n",
    "}\n",
    "\n",
    "data_list = []\n",
    "for _, row in df_train.iterrows():\n",
    "    graph = row[\"graph\"]\n",
    "    label_str = row[\"label_time\"]\n",
    "    # Convert the string label to a numerical value using the mapping\n",
    "    label_num = label_mapping[label_str]\n",
    "    # Assuming graph is a NetworkX graph\n",
    "    pyg_data = convert_nx_to_pyg_data(graph, label_num)\n",
    "    data_list.append(pyg_data)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_data, val_data = train_test_split(data_list, test_size=0.1, random_state=42)\n",
    "\n",
    "# Assuming you have a test dataset in a similar format\n",
    "test_data_list = []\n",
    "for _, row in df_test.iterrows():\n",
    "    graph = row[\"graph\"]\n",
    "    label_str = row[\"label_time\"]\n",
    "    label_num = label_mapping[label_str]\n",
    "    pyg_data = convert_nx_to_pyg_data(graph, label_num)\n",
    "    test_data_list.append(pyg_data)\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "test_dataloader = DataLoader(test_data_list, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11.5583,  0.7371],\n",
      "        [10.7485,  1.0607],\n",
      "        [ 6.1089,  5.3043],\n",
      "        [ 0.1407, 13.0577],\n",
      "        [ 2.8523,  3.9811],\n",
      "        [ 7.5872,  1.0717],\n",
      "        [ 0.8846,  3.4299],\n",
      "        [ 8.4719,  0.3661],\n",
      "        [ 0.9402,  0.2484],\n",
      "        [13.2872, 11.3556],\n",
      "        [ 7.2208, 12.8095],\n",
      "        [ 8.0238,  7.3646],\n",
      "        [ 8.3400, 12.2960],\n",
      "        [10.4877,  9.4897]])\n",
      "tensor([[ 0,  0,  1,  1,  4,  5,  6],\n",
      "        [ 4, 13, 12, 13,  6,  8,  8]])\n",
      "tensor([[0.1765],\n",
      "        [0.9303],\n",
      "        [0.3943],\n",
      "        [0.3801],\n",
      "        [0.1946],\n",
      "        [0.3115],\n",
      "        [0.2308]])\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0].x)\n",
    "print(data_list[0].edge_index)\n",
    "print(data_list[0].edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Features Shape: torch.Size([44, 2])\n",
      "Graph Connectivity Shape: torch.Size([2, 211])\n",
      "Edge Features Shape: torch.Size([211, 1])\n",
      "Target Label: 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Accessing the features of the first data instance\n",
    "first_data_instance = train_data[3]\n",
    "node_features = first_data_instance.x\n",
    "graph_connectivity = first_data_instance.edge_index\n",
    "edge_features = first_data_instance.edge_attr\n",
    "target_label = first_data_instance.y\n",
    "\n",
    "# Perform further analysis or operations as needed\n",
    "# For example, you can print the shapes of the tensors\n",
    "print(\"Node Features Shape:\", node_features.shape)\n",
    "print(\"Graph Connectivity Shape:\", graph_connectivity.shape)\n",
    "print(\"Edge Features Shape:\", edge_features.shape)\n",
    "print(\"Target Label:\", target_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the number or epochs and the learning rate\n",
    "num_epochs = 1000\n",
    "batch_size = 64\n",
    "mini_batch_size = 32\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] USING CPU\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(\"[i] USING CUDA\")\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "        print(\"[i] USING CPU\")\n",
    "    return device\n",
    "\n",
    "device = get_device() #setting up the DL device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "input_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "# define training function\n",
    "\n",
    "# implement early stopping for training function\n",
    "# from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss <= self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"[i] Validation Loss Increased - Early Stop!\")\n",
    "                print(\n",
    "                    f\"--- {validation_loss} > {self.min_validation_loss + self.min_delta} ---\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def train(net, train_loader, validation_loader, num_epochs, mini_batch_size, optimizer, lr_scheduler, criterion, earlystop_patience=0, earlystop_min_delta=1e-6, name=\"\"):\n",
    "    # Save the loss into a dataframe\n",
    "    losses = pd.DataFrame(index=list(range(num_epochs)), columns=[\n",
    "                          'running_loss', 'train_loss', 'valid_loss'])\n",
    "    min_validation_loss = np.inf\n",
    "\n",
    "    # Use a summary writer to check loss in real time\n",
    "    current_time = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    writer = SummaryWriter(\n",
    "        f'runs/tensorboard/{current_time}_{(net.__class__.__name__).lower()}_{name}')\n",
    "\n",
    "    # Set early stopping parameters\n",
    "    # from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "    early_stopping = EarlyStopper(\n",
    "        patience=earlystop_patience, min_delta=earlystop_min_delta)\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "\n",
    "    net.train()\n",
    "    net.to(device)  # Move the model to the specified device\n",
    "\n",
    "    for epoch in range(num_epochs):  # Looping over the dataset\n",
    "\n",
    "        running_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        net.train()  # Set the model to training mode\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            start_time_mini_batch = time.time()\n",
    "            inputs = data.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Setting the parameter gradients to zero\n",
    "            outputs = net(inputs)  # Forward pass\n",
    "\n",
    "            target_one_hot = F.one_hot(data.y, num_classes=num_classes)  # Adjust the number of classes accordingly\n",
    "\n",
    "            #print(target_one_hot, outputs)\n",
    "\n",
    "            loss = criterion(outputs, target_one_hot.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Optimization step\n",
    "\n",
    "            running_loss += loss.item()  # Updating the running loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if i % mini_batch_size == mini_batch_size - 1:  # Printing the running loss\n",
    "                print(f\"[epoch: {epoch + 1}, mini-batch: {i + 1}, time-taken: {round(time.time() - start_time_mini_batch, 3)} sec] loss: {round(running_loss / mini_batch_size, 6)} \")\n",
    "\n",
    "                # write on the summary writer\n",
    "                writer.add_scalar(\n",
    "                    'Loss/Running', running_loss / mini_batch_size, i)\n",
    "\n",
    "                running_loss = 0.0\n",
    "                start_time_mini_batch = time.time()\n",
    "\n",
    "\n",
    "        net.eval().to(device)\n",
    "\n",
    "        # Inside the validation loop\n",
    "        with torch.no_grad():\n",
    "            net.eval()  # Set the model to evaluation mode\n",
    "            all_labels = []\n",
    "            all_outputs = []\n",
    "\n",
    "            for i, data in enumerate(validation_loader):\n",
    "                inputs = data.to(device)\n",
    "\n",
    "                optimizer.zero_grad()  # Setting the parameter gradients to zero\n",
    "                outputs = net(inputs)  # Forward pass\n",
    "\n",
    "                target_one_hot = F.one_hot(data.y, num_classes=num_classes)  # Adjust the number of classes accordingly\n",
    "\n",
    "                loss = criterion(outputs, target_one_hot.float())\n",
    "\n",
    "                validation_loss += loss.item()  # Check the loss\n",
    "\n",
    "                # Use argmax to get the index of the predicted class\n",
    "                predicted_class = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                # Append predictions and labels for accuracy calculation\n",
    "                all_outputs.extend(predicted_class.cpu().numpy())\n",
    "                all_labels.extend(data.y.cpu().numpy())\n",
    "\n",
    "            # Convert lists to numpy arrays for easier computation\n",
    "            all_labels = np.array(all_labels)\n",
    "            all_outputs = np.array(all_outputs)\n",
    "\n",
    "            val_accuracy = accuracy_score(all_labels, all_outputs)\n",
    "            val_precision = precision_score(all_labels, all_outputs, average='weighted', zero_division=1.0)\n",
    "            val_recall = recall_score(all_labels, all_outputs, average='weighted', zero_division=1.0)\n",
    "            val_f1 = f1_score(all_labels, all_outputs, average='weighted', zero_division=1.0)\n",
    "\n",
    "            # Print or log the accuracy and validation loss\n",
    "            print(f'+++ [\\033[1mepoch: {epoch + 1}\\033[0m, validation - \\033[91maccuracy: {val_accuracy:.5f}\\033[0m, \\033[93mprecision: {val_precision:.5f}\\033[0m, \\033[94mrecall: {val_recall:.5f}\\033[0m, \\033[95mf1-score: {val_f1:.5f}\\033[0m] +++')\n",
    "\n",
    "        # Switch back to training mode for the next epoch\n",
    "        net.train().to(device)\n",
    "\n",
    "        print('+++ [epoch: %d, training loss: %.5f, validation loss: %.5f] +++' %\n",
    "              (epoch + 1,\n",
    "               train_loss / len(train_loader),\n",
    "               validation_loss / len(validation_loader)))\n",
    "\n",
    "        print(\n",
    "            f\"--- time-taken for epoch {epoch+1}: {round(time.time() - start_time_epoch, 3)} seconds ---\")\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        # Saving the loss\n",
    "        losses.at[epoch, 'running_loss'] = running_loss\n",
    "        losses.at[epoch, 'train_loss'] = train_loss\n",
    "        losses.at[epoch, 'valid_loss'] = validation_loss\n",
    "\n",
    "        # Write on the summary writer\n",
    "        writer.add_scalar('Loss/Train', train_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar('Loss/Validation', validation_loss /\n",
    "                          len(validation_loader), epoch)\n",
    "\n",
    "        # Update the learning rate\n",
    "        if lr_scheduler.__class__.__name__ == \"CosineAnnealingWarmRestarts\" and lr_scheduler is not None:\n",
    "            print(f\"\\033[90m--- current LR: {round(lr_scheduler.get_last_lr()[0], 9)} ---\\033[0m\")\n",
    "            lr_scheduler.step()  # step scheduler learning rate\n",
    "\n",
    "        if min_validation_loss > (validation_loss / len(validation_loader)):\n",
    "            print(f'\\033[92m+++ [validation loss decreased ({min_validation_loss:.9f} -> {(validation_loss / len(validation_loader)):.9f}), saving the model ...] +++\\033[0m')\n",
    "            min_validation_loss = validation_loss / len(validation_loader)\n",
    "\n",
    "            # Check if the directory exists, and if not, create it\n",
    "            save_dir = f'./runs/models/{(net.__class__.__name__).lower()}'\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            # Save State Dict\n",
    "            torch.save(net.state_dict(), f'{save_dir}/{(net.__class__.__name__).lower()}_{name}_saved_model.pth')\n",
    "\n",
    "        # Check if early stopping criteria is fulfilled\n",
    "        if early_stopping.early_stop(validation_loss):\n",
    "            break\n",
    "\n",
    "    pickle.dump(losses, open(\n",
    "        f'./runs/models/{(net.__class__.__name__).lower()}/{(net.__class__.__name__).lower()}_{name}_loss.pkl', 'wb'))\n",
    "    writer.close()\n",
    "    print(f\"[i] Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "\n",
    "class ComplexGNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, init_fn):\n",
    "        super(ComplexGNN, self).__init__()\n",
    "        hidden_dim1 = 1024\n",
    "        hidden_dim2 = 512\n",
    "        hidden_dim3 = 256\n",
    "        hidden_dim4 = 128\n",
    "        hidden_dim5 = 64\n",
    "\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim1)\n",
    "        self.bn1 = BatchNorm(hidden_dim1)\n",
    "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = BatchNorm(hidden_dim2)\n",
    "        self.conv3 = GCNConv(hidden_dim2, hidden_dim3)\n",
    "        self.bn3 = BatchNorm(hidden_dim3)\n",
    "        self.conv4 = GCNConv(hidden_dim3, hidden_dim4)\n",
    "        self.bn4 = BatchNorm(hidden_dim4)\n",
    "        self.conv5 = GCNConv(hidden_dim4, hidden_dim5)\n",
    "        self.bn5 = BatchNorm(hidden_dim5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        fc_hidden_dim1 = 64\n",
    "        fc_hidden_dim2 = 32\n",
    "        fc_hidden_dim3 = 16 \n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim5, fc_hidden_dim1)\n",
    "        self.fc2 = nn.Linear(fc_hidden_dim1, fc_hidden_dim2)\n",
    "        self.fc3 = nn.Linear(fc_hidden_dim2, fc_hidden_dim3)\n",
    "        self.fc4 = nn.Linear(fc_hidden_dim3, output_dim)\n",
    "\n",
    "        self.init_fn = init_fn\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            self.init_fn(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Ensure edge_index has the correct shape\n",
    "        edge_index = data.edge_index.view(2, -1)\n",
    "\n",
    "        # Extract relevant data\n",
    "        x, batch, edge_attr = data.x, data.batch, data.edge_attr\n",
    "\n",
    "        # Apply graph convolutional layers with batch normalization\n",
    "        x = F.elu(self.bn1(self.conv1(x, edge_index)))\n",
    "        x = F.elu(self.bn2(self.conv2(x, edge_index)))\n",
    "        x = F.elu(self.bn3(self.conv3(x, edge_index)))\n",
    "        x = F.elu(self.bn4(self.conv4(x, edge_index)))\n",
    "        x = F.elu(self.bn5(self.conv5(x, edge_index)))\n",
    "\n",
    "        # Global pooling to obtain a representation for each graph\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Training the network ComplexGNN ...\n",
      "+++ [\u001b[1mepoch: 1\u001b[0m, validation - \u001b[91maccuracy: 0.63889\u001b[0m, \u001b[93mprecision: 0.66890\u001b[0m, \u001b[94mrecall: 0.63889\u001b[0m, \u001b[95mf1-score: 0.50889\u001b[0m] +++\n",
      "+++ [epoch: 1, training loss: 1.57004, validation loss: 1.42927] +++\n",
      "--- time-taken for epoch 1: 13.0 seconds ---\n",
      "\u001b[90m--- current LR: 0.003 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (inf -> 1.429269711), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 2\u001b[0m, validation - \u001b[91maccuracy: 0.63889\u001b[0m, \u001b[93mprecision: 0.76929\u001b[0m, \u001b[94mrecall: 0.63889\u001b[0m, \u001b[95mf1-score: 0.49812\u001b[0m] +++\n",
      "+++ [epoch: 2, training loss: 1.02655, validation loss: 1.50627] +++\n",
      "--- time-taken for epoch 2: 13.585 seconds ---\n",
      "\u001b[90m--- current LR: 0.002799038 ---\u001b[0m\n",
      "[i] Validation Loss Increased - Early Stop!\n",
      "--- 4.518815755844116 > 4.287810133529663 ---\n",
      "[i] Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = ComplexGNN(input_dim=2, output_dim=num_classes, init_fn=torch.nn.init.xavier_normal_).to(device)  # Adjust output_dim based on your task\n",
    "train_flag = False # Dont run if False\n",
    "name = \"gnn_complex\"\n",
    "\n",
    "if train_flag:\n",
    "    print(f\"[i] Training the network {model.__class__.__name__} ...\")\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate = 3e-3\n",
    "    \n",
    "    # Define your criterion (e.g., CrossEntropyLoss for multiclass classification)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Define your optimizer (e.g., Adam)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Cosine Annealing with Restarts (CWR) scheduler.\n",
    "    # This scheduler is designed to automatically adjust the learning rate according to a cosine wave, and can be used to adjust the learning rate as the model converges.\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=round((1/10)*batch_size), T_mult=1, eta_min=0)\n",
    "\n",
    "    # Train the model\n",
    "    train(model, train_dataloader, val_dataloader, num_epochs, mini_batch_size, optimizer, scheduler, criterion, name=name)\n",
    "else:  # load the model\n",
    "    print(f\"[i] Loading the network {model.__class__.__name__} ...\")\n",
    "    # Loading existing models (with saved weights)\n",
    "    model.load_state_dict(torch.load(f'./runs/models/{(model.__class__.__name__).lower()}/{(model.__class__.__name__).lower()}_{name}_saved_model.pth', map_location=device))  # using saved data if present\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "+++ [\u001b[0m, validation - \u001b[91maccuracy: 0.65000\u001b[0m, \u001b[93mprecision: 0.77250\u001b[0m, \u001b[94mrecall: 0.65000\u001b[0m, \u001b[95mf1-score: 0.51212\u001b[0m] +++\n",
      "[[0.   0.   0.04 0.   0.   0.  ]\n",
      " [0.   0.   0.14 0.   0.   0.  ]\n",
      " [0.   0.   0.65 0.   0.   0.  ]\n",
      " [0.   0.   0.13 0.   0.   0.  ]\n",
      " [0.   0.   0.02 0.   0.   0.  ]\n",
      " [0.   0.   0.02 0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store true labels and predicted labels\n",
    "all_labels = []\n",
    "all_outputs = []\n",
    "\n",
    "# Loop through the test dataset\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        inputs = data.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        target_one_hot = F.one_hot(data.y, num_classes=num_classes)\n",
    "        loss = criterion(outputs, target_one_hot.float())\n",
    "\n",
    "        predicted_class = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Append predictions and labels for accuracy calculation\n",
    "        all_outputs.extend(predicted_class.cpu().numpy())\n",
    "        all_labels.extend(data.y.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays for easier computation\n",
    "all_labels = np.array(all_labels)\n",
    "all_outputs = np.array(all_outputs)\n",
    "\n",
    "print(all_outputs)\n",
    "val_accuracy = accuracy_score(all_labels, all_outputs)\n",
    "val_precision = precision_score(all_labels, all_outputs, average='weighted', zero_division=1.0)\n",
    "val_recall = recall_score(all_labels, all_outputs, average='weighted', zero_division=1.0)\n",
    "val_f1 = f1_score(all_labels, all_outputs, average='weighted', zero_division=1.0)\n",
    "\n",
    "print(f'+++ [\\033[0m, validation - \\033[91maccuracy: {val_accuracy:.5f}\\033[0m, \\033[93mprecision: {val_precision:.5f}\\033[0m, \\033[94mrecall: {val_recall:.5f}\\033[0m, \\033[95mf1-score: {val_f1:.5f}\\033[0m] +++')\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_outputs, normalize=\"pred\")\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
