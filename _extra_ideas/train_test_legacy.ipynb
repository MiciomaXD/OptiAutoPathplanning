{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, csv file I/O (e.g. pd.read_csv)\n",
    "import os # deal with os primitives\n",
    "import time # time-related functions\n",
    "import matplotlib.pyplot as plt # plots\n",
    "import pickle # object serialization\n",
    "from collections import Counter # dict subclass for counting hashable items\n",
    "from tqdm import tqdm # iterable object\n",
    "import random # random values generator\n",
    "import seaborn as sns # prettier plots\n",
    "import torch # main package for PyTorch\n",
    "import torch.utils.data as data_utils # access data sets, including pre-processing, loading, and splitting\n",
    "from torch.utils.data import random_split # randomly split a dataset \n",
    "import torch.optim as optim # optimization algorithms\n",
    "import torch.nn as nn # build neural network (layers, activations, loss functions)\n",
    "import torch.nn.functional as F # functions used to build neural network\n",
    "from torchsummary import summary # print the summary of a neural network model\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts # scheduler used to adjust the learning rate\n",
    "from torch.utils.tensorboard.writer import SummaryWriter # nn log writer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "torch.manual_seed(1111)\n",
    "torch.cuda.manual_seed(1111)\n",
    "np.random.seed(1111)\n",
    "random.seed(1111)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First matrix in train set:\n",
      "Graph with 18 nodes and 29 edges\n",
      "\n",
      "First target in train set:\n",
      "best_first_search\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the train set\n",
    "df_train = pd.read_pickle(\"df_train.pkl\")\n",
    "\n",
    "# Load the test set\n",
    "df_test = pd.read_pickle(\"df_test.pkl\")\n",
    "\n",
    "# Define a function to extract the matrix and target from a DataFrame\n",
    "def extract_matrix_and_target(df):\n",
    "    matrix_column = \"graph\"\n",
    "    target_column = \"label_time\"\n",
    "\n",
    "    matrices = df[matrix_column].tolist()\n",
    "    targets = df[target_column].tolist()\n",
    "\n",
    "    return matrices, targets\n",
    "\n",
    "# Extract matrix and target for train set\n",
    "train_graphs, train_targets = extract_matrix_and_target(df_train)\n",
    "\n",
    "# Extract matrix and target for test set\n",
    "test_graphs, test_targets = extract_matrix_and_target(df_test)\n",
    "\n",
    "# Display the first matrix and target in the train set\n",
    "print(\"First matrix in train set:\")\n",
    "print(train_graphs[0])\n",
    "\n",
    "print(\"\\nFirst target in train set:\")\n",
    "print(train_targets[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "def calculate_graph_size_features(graph):\n",
    "    num_nodes = len(graph.nodes)\n",
    "    num_edges = len(graph.edges)\n",
    "\n",
    "    return {\n",
    "        'nodes': num_nodes,\n",
    "        'edges': num_edges,\n",
    "        'ratio_n_m': num_nodes / num_edges if num_edges != 0 else 0,\n",
    "        'ratio_m_n': num_edges / num_nodes if num_nodes != 0 else 0,\n",
    "        'density': 2 * num_edges / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0\n",
    "    }\n",
    "\n",
    "def calculate_node_degree_stats(graph):\n",
    "    degrees = list(dict(graph.degree()).values())\n",
    "    degrees_np = np.array(degrees)\n",
    "\n",
    "    unique_degrees, counts = np.unique(degrees_np, return_counts=True)\n",
    "\n",
    "    entropy_term = np.fromiter((p * np.log2(p) for p in (np.count_nonzero(degrees_np == x) / len(degrees_np) for x in set(degrees_np))), dtype=float)\n",
    "    \n",
    "    return {\n",
    "        'min_degree': np.min(degrees_np),\n",
    "        'max_degree': np.max(degrees_np),\n",
    "        'mean_degree': np.mean(degrees_np),\n",
    "        'median_degree': np.median(degrees_np),\n",
    "        'q0.25_degree': np.percentile(degrees_np, 25),\n",
    "        'q0.75_degree': np.percentile(degrees_np, 75),\n",
    "        'variation_coefficient_degree': np.std(degrees_np) / np.mean(degrees_np),\n",
    "        'entropy_degree': -np.sum(entropy_term)\n",
    "    }\n",
    "\n",
    "def calculate_maximal_clique_stats(graph, num_nodes):\n",
    "    cliques = list(map(len, nx.enumerate_all_cliques(graph)))\n",
    "    return {\n",
    "        'normalized_min_clique_size': min(cliques) / num_nodes,\n",
    "        'normalized_max_clique_size': max(cliques) / num_nodes,\n",
    "        'normalized_median_clique_size': statistics.median(cliques) / num_nodes,\n",
    "        'normalized_q0.25_clique_size': statistics.quantiles(cliques, n=4)[0] / num_nodes,\n",
    "        'normalized_q0.75_clique_size': statistics.quantiles(cliques, n=4)[2] / num_nodes,\n",
    "        'normalized_variation_coefficient_clique_size': statistics.stdev(cliques) / statistics.mean(cliques),\n",
    "        'normalized_entropy_clique_size': -sum(p * math.log2(p) for p in (cliques.count(x) / len(cliques) for x in set(cliques))) / num_nodes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_graph_size_features(graph):\n",
    "    num_nodes = len(graph.nodes)\n",
    "    num_edges = len(graph.edges)\n",
    "\n",
    "    # Check for division by zero\n",
    "    if num_edges != 0 and num_nodes != 0:\n",
    "        return {\n",
    "            'nodes': num_nodes,\n",
    "            'edges': num_edges,\n",
    "            'ratio_n_m': num_nodes / num_edges,\n",
    "            'ratio_m_n': num_edges / num_nodes,\n",
    "            'density': 2 * num_edges / (num_nodes * (num_nodes - 1))\n",
    "        }\n",
    "    else:\n",
    "        # Handle the case where either num_edges or num_nodes is zero\n",
    "        return {\n",
    "            'nodes': num_nodes,\n",
    "            'edges': num_edges,\n",
    "            'ratio_n_m': 0,\n",
    "            'ratio_m_n': 0,\n",
    "            'density': 0\n",
    "        }\n",
    "\n",
    "def calculate_node_degree_stats(graph):\n",
    "    degrees = list(dict(graph.degree()).values())\n",
    "    degrees_np = np.array(degrees)\n",
    "\n",
    "    # Check for diversity in degrees\n",
    "    if len(set(degrees_np)) > 1:\n",
    "        unique_degrees, counts = np.unique(degrees_np, return_counts=True)\n",
    "\n",
    "        entropy_term = np.fromiter((p * np.log2(p) for p in (np.count_nonzero(degrees_np == x) / len(degrees_np) for x in set(degrees_np))), dtype=float)\n",
    "\n",
    "        return {\n",
    "            'min_degree': np.min(degrees_np),\n",
    "            'max_degree': np.max(degrees_np),\n",
    "            'mean_degree': np.mean(degrees_np),\n",
    "            'median_degree': np.median(degrees_np),\n",
    "            'q0.25_degree': np.percentile(degrees_np, 25),\n",
    "            'q0.75_degree': np.percentile(degrees_np, 75),\n",
    "            'variation_coefficient_degree': np.std(degrees_np) / np.mean(degrees_np),\n",
    "            'entropy_degree': -np.sum(entropy_term)\n",
    "        }\n",
    "    else:\n",
    "        # Handle the case where there is no diversity in degrees\n",
    "        return {\n",
    "            'min_degree': 0,\n",
    "            'max_degree': 0,\n",
    "            'mean_degree': 0,\n",
    "            'median_degree': 0,\n",
    "            'q0.25_degree': 0,\n",
    "            'q0.75_degree': 0,\n",
    "            'variation_coefficient_degree': 0,\n",
    "            'entropy_degree': 0\n",
    "        }\n",
    "\n",
    "\n",
    "def calculate_maximal_clique_stats(graph, num_nodes):\n",
    "    cliques = list(map(len, nx.enumerate_all_cliques(graph)))\n",
    "\n",
    "    # Check for diversity in cliques\n",
    "    if len(set(cliques)) > 1:\n",
    "        return {\n",
    "            'normalized_min_clique_size': min(cliques) / num_nodes,\n",
    "            'normalized_max_clique_size': max(cliques) / num_nodes,\n",
    "            'normalized_median_clique_size': statistics.median(cliques) / num_nodes,\n",
    "            'normalized_q0.25_clique_size': statistics.quantiles(cliques, n=4)[0] / num_nodes,\n",
    "            'normalized_q0.75_clique_size': statistics.quantiles(cliques, n=4)[2] / num_nodes,\n",
    "            'normalized_variation_coefficient_clique_size': statistics.stdev(cliques) / statistics.mean(cliques),\n",
    "            'normalized_entropy_clique_size': -sum(p * math.log2(p) for p in (cliques.count(x) / len(cliques) for x in set(cliques))) / num_nodes\n",
    "        }\n",
    "    else:\n",
    "        # Handle the case where there is no diversity in cliques\n",
    "        return {\n",
    "            'normalized_min_clique_size': 0,\n",
    "            'normalized_max_clique_size': 0,\n",
    "            'normalized_median_clique_size': 0,\n",
    "            'normalized_q0.25_clique_size': 0,\n",
    "            'normalized_q0.75_clique_size': 0,\n",
    "            'normalized_variation_coefficient_clique_size': 0,\n",
    "            'normalized_entropy_clique_size': 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Biagio Tomasetig\\AppData\\Local\\Temp\\ipykernel_16908\\3202479456.py:33: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  'variation_coefficient_degree': np.std(degrees_np) / np.mean(degrees_np),\n"
     ]
    }
   ],
   "source": [
    "# Loop through each graph in the train set\n",
    "train_graph_metrics = []\n",
    "for graph, target in zip(train_graphs, train_targets):\n",
    "    size_features = calculate_graph_size_features(graph)\n",
    "    degree_stats = calculate_node_degree_stats(graph)\n",
    "    clique_stats = calculate_maximal_clique_stats(graph, len(graph.nodes()))\n",
    "\n",
    "    # Combine the metrics into one dictionary\n",
    "    graph_metrics = {**size_features, **degree_stats, **clique_stats, 'target': target}\n",
    "    \n",
    "    train_graph_metrics.append(graph_metrics)\n",
    "\n",
    "# Loop through each graph in the test set\n",
    "test_graph_metrics = []\n",
    "for graph, target in zip(test_graphs, test_targets):\n",
    "    graph_array = nx.to_numpy_array(graph)\n",
    "    size_features = calculate_graph_size_features(graph)\n",
    "    degree_stats = calculate_node_degree_stats(graph)\n",
    "    clique_stats = calculate_maximal_clique_stats(graph, graph_array.shape[0])  # Use graph_array.shape[0] for the number of nodes\n",
    "    \n",
    "    # Combine the metrics into one dictionary\n",
    "    graph_metrics = {**size_features, **degree_stats, **clique_stats, 'target': target}\n",
    "    \n",
    "    test_graph_metrics.append(graph_metrics)\n",
    "\n",
    "# Convert train_graph_metrics and test_graph_metrics to DataFrames\n",
    "train_df = pd.DataFrame(train_graph_metrics)\n",
    "test_df = pd.DataFrame(test_graph_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>edges</th>\n",
       "      <th>ratio_n_m</th>\n",
       "      <th>ratio_m_n</th>\n",
       "      <th>density</th>\n",
       "      <th>min_degree</th>\n",
       "      <th>max_degree</th>\n",
       "      <th>mean_degree</th>\n",
       "      <th>median_degree</th>\n",
       "      <th>q0.25_degree</th>\n",
       "      <th>...</th>\n",
       "      <th>variation_coefficient_degree</th>\n",
       "      <th>entropy_degree</th>\n",
       "      <th>normalized_min_clique_size</th>\n",
       "      <th>normalized_max_clique_size</th>\n",
       "      <th>normalized_median_clique_size</th>\n",
       "      <th>normalized_q0.25_clique_size</th>\n",
       "      <th>normalized_q0.75_clique_size</th>\n",
       "      <th>normalized_variation_coefficient_clique_size</th>\n",
       "      <th>normalized_entropy_clique_size</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>1.611111</td>\n",
       "      <td>0.189542</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491304</td>\n",
       "      <td>2.530493</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.364807</td>\n",
       "      <td>0.077338</td>\n",
       "      <td>best_first_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>108</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>2.769231</td>\n",
       "      <td>0.145749</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5.538462</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372160</td>\n",
       "      <td>2.820963</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.326926</td>\n",
       "      <td>0.035698</td>\n",
       "      <td>best_first_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.230769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.910014</td>\n",
       "      <td>1.884314</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.360341</td>\n",
       "      <td>0.073747</td>\n",
       "      <td>dfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>115</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>3.194444</td>\n",
       "      <td>0.182540</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>6.388889</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342679</td>\n",
       "      <td>2.971077</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.316642</td>\n",
       "      <td>0.038383</td>\n",
       "      <td>best_first_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>121</td>\n",
       "      <td>0.264463</td>\n",
       "      <td>3.781250</td>\n",
       "      <td>0.243952</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>7.562500</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260167</td>\n",
       "      <td>2.885391</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.306671</td>\n",
       "      <td>0.044608</td>\n",
       "      <td>best_first_search</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   nodes  edges  ratio_n_m  ratio_m_n   density  min_degree  max_degree  \\\n",
       "0     18     29   0.620690   1.611111  0.189542           1           6   \n",
       "1     39    108   0.361111   2.769231  0.145749           2          11   \n",
       "2     13      8   1.625000   0.615385  0.102564           0           3   \n",
       "3     36    115   0.313043   3.194444  0.182540           3          12   \n",
       "4     32    121   0.264463   3.781250  0.243952           4          12   \n",
       "\n",
       "   mean_degree  median_degree  q0.25_degree  ...  \\\n",
       "0     3.222222            3.0           2.0  ...   \n",
       "1     5.538462            5.0           4.0  ...   \n",
       "2     1.230769            1.0           0.0  ...   \n",
       "3     6.388889            6.0           5.0  ...   \n",
       "4     7.562500            8.0           6.0  ...   \n",
       "\n",
       "   variation_coefficient_degree  entropy_degree  normalized_min_clique_size  \\\n",
       "0                      0.491304        2.530493                    0.055556   \n",
       "1                      0.372160        2.820963                    0.025641   \n",
       "2                      0.910014        1.884314                    0.076923   \n",
       "3                      0.342679        2.971077                    0.027778   \n",
       "4                      0.260167        2.885391                    0.031250   \n",
       "\n",
       "   normalized_max_clique_size  normalized_median_clique_size  \\\n",
       "0                    0.166667                       0.111111   \n",
       "1                    0.102564                       0.051282   \n",
       "2                    0.153846                       0.076923   \n",
       "3                    0.111111                       0.055556   \n",
       "4                    0.125000                       0.062500   \n",
       "\n",
       "   normalized_q0.25_clique_size  normalized_q0.75_clique_size  \\\n",
       "0                      0.055556                      0.111111   \n",
       "1                      0.051282                      0.051282   \n",
       "2                      0.076923                      0.153846   \n",
       "3                      0.055556                      0.055556   \n",
       "4                      0.062500                      0.093750   \n",
       "\n",
       "   normalized_variation_coefficient_clique_size  \\\n",
       "0                                      0.364807   \n",
       "1                                      0.326926   \n",
       "2                                      0.360341   \n",
       "3                                      0.316642   \n",
       "4                                      0.306671   \n",
       "\n",
       "   normalized_entropy_clique_size             target  \n",
       "0                        0.077338  best_first_search  \n",
       "1                        0.035698  best_first_search  \n",
       "2                        0.073747                dfs  \n",
       "3                        0.038383  best_first_search  \n",
       "4                        0.044608  best_first_search  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of each DataFrame for verification\n",
    "print(\"Train DataFrame:\")\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>edges</th>\n",
       "      <th>ratio_n_m</th>\n",
       "      <th>ratio_m_n</th>\n",
       "      <th>density</th>\n",
       "      <th>min_degree</th>\n",
       "      <th>max_degree</th>\n",
       "      <th>mean_degree</th>\n",
       "      <th>median_degree</th>\n",
       "      <th>q0.25_degree</th>\n",
       "      <th>...</th>\n",
       "      <th>variation_coefficient_degree</th>\n",
       "      <th>entropy_degree</th>\n",
       "      <th>normalized_min_clique_size</th>\n",
       "      <th>normalized_max_clique_size</th>\n",
       "      <th>normalized_median_clique_size</th>\n",
       "      <th>normalized_q0.25_clique_size</th>\n",
       "      <th>normalized_q0.75_clique_size</th>\n",
       "      <th>normalized_variation_coefficient_clique_size</th>\n",
       "      <th>normalized_entropy_clique_size</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.118034</td>\n",
       "      <td>0.991076</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.342286</td>\n",
       "      <td>0.076004</td>\n",
       "      <td>dfs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>41</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>1.782609</td>\n",
       "      <td>0.162055</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3.565217</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451714</td>\n",
       "      <td>2.522866</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.349720</td>\n",
       "      <td>0.058293</td>\n",
       "      <td>best_first_search</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>randomized_shortest_path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>randomized_shortest_path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>71</td>\n",
       "      <td>0.450704</td>\n",
       "      <td>2.218750</td>\n",
       "      <td>0.143145</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373439</td>\n",
       "      <td>2.565206</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.321445</td>\n",
       "      <td>0.038946</td>\n",
       "      <td>best_first_search</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   nodes  edges  ratio_n_m  ratio_m_n   density  min_degree  max_degree  \\\n",
       "0      9      2   4.500000   0.222222  0.055556           0           1   \n",
       "1     23     41   0.560976   1.782609  0.162055           1           7   \n",
       "2      5      0   0.000000   0.000000  0.000000           0           0   \n",
       "3     11      0   0.000000   0.000000  0.000000           0           0   \n",
       "4     32     71   0.450704   2.218750  0.143145           1           7   \n",
       "\n",
       "   mean_degree  median_degree  q0.25_degree  ...  \\\n",
       "0     0.444444            0.0           0.0  ...   \n",
       "1     3.565217            4.0           2.0  ...   \n",
       "2     0.000000            0.0           0.0  ...   \n",
       "3     0.000000            0.0           0.0  ...   \n",
       "4     4.437500            4.0           3.0  ...   \n",
       "\n",
       "   variation_coefficient_degree  entropy_degree  normalized_min_clique_size  \\\n",
       "0                      1.118034        0.991076                    0.111111   \n",
       "1                      0.451714        2.522866                    0.043478   \n",
       "2                           NaN       -0.000000                    0.200000   \n",
       "3                           NaN       -0.000000                    0.090909   \n",
       "4                      0.373439        2.565206                    0.031250   \n",
       "\n",
       "   normalized_max_clique_size  normalized_median_clique_size  \\\n",
       "0                    0.222222                       0.111111   \n",
       "1                    0.130435                       0.086957   \n",
       "2                    0.200000                       0.200000   \n",
       "3                    0.090909                       0.090909   \n",
       "4                    0.093750                       0.062500   \n",
       "\n",
       "   normalized_q0.25_clique_size  normalized_q0.75_clique_size  \\\n",
       "0                      0.111111                      0.111111   \n",
       "1                      0.043478                      0.086957   \n",
       "2                      0.200000                      0.200000   \n",
       "3                      0.090909                      0.090909   \n",
       "4                      0.031250                      0.062500   \n",
       "\n",
       "   normalized_variation_coefficient_clique_size  \\\n",
       "0                                      0.342286   \n",
       "1                                      0.349720   \n",
       "2                                      0.000000   \n",
       "3                                      0.000000   \n",
       "4                                      0.321445   \n",
       "\n",
       "   normalized_entropy_clique_size                    target  \n",
       "0                        0.076004                       dfs  \n",
       "1                        0.058293         best_first_search  \n",
       "2                       -0.000000  randomized_shortest_path  \n",
       "3                       -0.000000  randomized_shortest_path  \n",
       "4                        0.038946         best_first_search  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTest DataFrame:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nodes', 'edges', 'ratio_n_m', 'ratio_m_n', 'density', 'min_degree',\n",
       "       'max_degree', 'mean_degree', 'median_degree', 'q0.25_degree',\n",
       "       'q0.75_degree', 'variation_coefficient_degree', 'entropy_degree',\n",
       "       'normalized_min_clique_size', 'normalized_max_clique_size',\n",
       "       'normalized_median_clique_size', 'normalized_q0.25_clique_size',\n",
       "       'normalized_q0.75_clique_size',\n",
       "       'normalized_variation_coefficient_clique_size',\n",
       "       'normalized_entropy_clique_size', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] USING CPU\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(\"[i] USING CUDA\")\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "        print(\"[i] USING CPU\")\n",
    "    return device\n",
    "\n",
    "device = get_device() #setting up the DL device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the number or epochs and the learning rate\n",
    "num_epochs = 1000\n",
    "batch_size = 256\n",
    "mini_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "\n",
    "# Assuming train_df and test_df are your dataframes\n",
    "# Assuming 'target' is your target column name\n",
    "\n",
    "# Combine train and test datasets for uniform preprocessing\n",
    "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = combined_df.drop('target', axis=1)\n",
    "y = combined_df['target']\n",
    "\n",
    "# Define features to be normalized and one-hot encoded\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Define preprocessing steps for numeric and categorical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit and transform data\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "# 90 train 10 validation\n",
    "# 20% test\n",
    "X_train_temp, X_test, y_train_temp, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# One-hot encode the target column\n",
    "y_train_onehot = pd.get_dummies(y_train)\n",
    "y_val_onehot = pd.get_dummies(y_val)\n",
    "y_test_onehot = pd.get_dummies(y_test)\n",
    "\n",
    "# Convert one-hot encoded targets to PyTorch tensors\n",
    "y_train_tensor = torch.tensor(y_train_onehot.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_onehot.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_onehot.values, dtype=torch.float32)\n",
    "\n",
    "# Calculate class weights for the training set\n",
    "class_sample_count = np.array([len(np.where(y_train_onehot.iloc[:, c] == 1)[0]) for c in range(y_train_onehot.shape[1])])\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[c] for c in range(y_train_onehot.shape[1])])\n",
    "\n",
    "# Create a WeightedRandomSampler for the training set\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create PyTorch dataloaders with WeightedRandomSampler for the training set\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, sampler=sampler)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "input_size = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Class Counts: [372, 55, 52, 194, 13, 34]\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters for each class\n",
    "class_counts = [0] * y_train_tensor.shape[1]\n",
    "\n",
    "# Iterate through the entire training dataset without using the sampler\n",
    "for batch_idx, (_, y_one_hot) in enumerate(DataLoader(train_dataset, batch_size=batch_size, shuffle=False)):\n",
    "    # Convert one-hot encoded labels to class indices\n",
    "    y = torch.argmax(y_one_hot, dim=1)\n",
    "\n",
    "    # Update class counts for each batch\n",
    "    for class_idx in range(y_train_tensor.shape[1]):\n",
    "        class_counts[class_idx] += len(torch.where(y == class_idx)[0])\n",
    "\n",
    "# Print cumulative count of elements in different classes\n",
    "print(\"Cumulative Class Counts:\", class_counts)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# define training function\n",
    "\n",
    "# implement early stopping for training function\n",
    "# from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss <= self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"[i] Validation Loss Increased - Early Stop!\")\n",
    "                print(\n",
    "                    f\"--- {validation_loss} > {self.min_validation_loss + self.min_delta} ---\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def train(net, train_loader, validation_loader, num_epochs, batch_size, mini_batch_size, optimizer, lr_scheduler, criterion, earlystop_patience=0, earlystop_min_delta=1e-6, name=\"\"):\n",
    "    # Save the loss into a dataframe\n",
    "    losses = pd.DataFrame(index=list(range(num_epochs)), columns=[\n",
    "                          'running_loss', 'train_loss', 'valid_loss'])\n",
    "    min_validation_loss = np.inf\n",
    "\n",
    "    # Use a summary writer to check loss in real time\n",
    "    current_time = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    writer = SummaryWriter(\n",
    "        f'runs/tensorboard/{current_time}_{(net.__class__.__name__).lower()}_{name}')\n",
    "\n",
    "    # Set early stopping parameters\n",
    "    # from https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch\n",
    "    early_stopping = EarlyStopper(\n",
    "        patience=earlystop_patience, min_delta=earlystop_min_delta)\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "\n",
    "    net.train()\n",
    "    net.to(device)  # Move the model to the specified device\n",
    "\n",
    "    for epoch in range(num_epochs):  # Looping over the dataset\n",
    "\n",
    "        running_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        \n",
    "\n",
    "        net.train()  # Set the model to training mode\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            start_time_mini_batch = time.time()\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Setting the parameter gradients to zero\n",
    "            outputs = net(inputs)  # Forward pass\n",
    "\n",
    "            labels = labels.float()\n",
    "\n",
    "            loss = criterion(outputs, labels)  # Applying the criterion\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Optimization step\n",
    "\n",
    "            running_loss += loss.item()  # Updating the running loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if i % mini_batch_size == mini_batch_size - 1:  # Printing the running loss\n",
    "                print(f\"[epoch: {epoch + 1}, mini-batch: {i + 1}, time-taken: {round(time.time() - start_time_mini_batch, 3)} sec] loss: {round(running_loss / mini_batch_size, 6)} \")\n",
    "\n",
    "                # write on the summary writer\n",
    "                writer.add_scalar(\n",
    "                    'Loss/Running', running_loss / mini_batch_size, i)\n",
    "\n",
    "                running_loss = 0.0\n",
    "                start_time_mini_batch = time.time()\n",
    "\n",
    "\n",
    "        net.eval().to(device)\n",
    "\n",
    "        # Inside the validation loop\n",
    "        with torch.no_grad():\n",
    "            net.eval()  # Set the model to evaluation mode\n",
    "            all_labels = []\n",
    "            all_outputs = []\n",
    "\n",
    "            for i, data in enumerate(validation_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs)  # Forward pass\n",
    "\n",
    "                loss = criterion(outputs, labels) # Applying the criterion\n",
    "                validation_loss += loss.item() # Check the loss\n",
    "\n",
    "                # Convert probabilities to binary predictions using a threshold (e.g., 0.5)\n",
    "                threshold = 0.5\n",
    "                binary_predictions = (outputs > threshold).float()\n",
    "\n",
    "                # Append predictions and labels for accuracy calculation\n",
    "                all_outputs.extend(binary_predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Convert lists to numpy arrays for easier computation\n",
    "            all_labels = np.array(all_labels)\n",
    "            all_outputs = np.array(all_outputs)\n",
    "\n",
    "            # Compute confusion matrix\n",
    "            conf_matrix = confusion_matrix(\n",
    "                all_labels.flatten(), all_outputs.flatten())\n",
    "\n",
    "            # Extract TP, TN, FP, FN from the confusion matrix\n",
    "            TP = conf_matrix[1, 1]\n",
    "            TN = conf_matrix[0, 0]\n",
    "            FP = conf_matrix[0, 1]\n",
    "            FN = conf_matrix[1, 0]\n",
    "\n",
    "            # Check if denominators are zero\n",
    "            precision_denominator = TP + FP\n",
    "            recall_denominator = TP + FN\n",
    "\n",
    "            # Calculate accuracy, precision, recall, and F1-score\n",
    "            validation_accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "            # Check if the denominator is not zero before performing division\n",
    "            validation_precision = TP / precision_denominator if precision_denominator != 0 else 0\n",
    "            validation_recall = TP / recall_denominator if recall_denominator != 0 else 0\n",
    "\n",
    "            # Check if both precision and recall are not zero before performing division\n",
    "            if validation_precision + validation_recall != 0:\n",
    "                validation_f1_score = 2 * (validation_precision * validation_recall) / (validation_precision + validation_recall)\n",
    "            else:\n",
    "                validation_f1_score = 0\n",
    "\n",
    "            # Print or log the accuracy and validation loss\n",
    "            print(f'+++ [\\033[1mepoch: {epoch + 1}\\033[0m, validation - \\033[91maccuracy: {validation_accuracy:.5f}\\033[0m, \\033[93mprecision: {validation_precision:.5f}\\033[0m, \\033[94mrecall: {validation_recall:.5f}\\033[0m, \\033[95mf1-score: {validation_f1_score:.5f}\\033[0m] +++')\n",
    "\n",
    "        # Switch back to training mode for the next epoch\n",
    "        net.train().to(device)\n",
    "\n",
    "        print('+++ [epoch: %d, training loss: %.5f, validation loss: %.5f] +++' %\n",
    "              (epoch + 1,\n",
    "               train_loss / len(train_loader),\n",
    "               validation_loss / len(validation_loader)))\n",
    "\n",
    "        print(\n",
    "            f\"--- time-taken for epoch {epoch+1}: {round(time.time() - start_time_epoch, 3)} seconds ---\")\n",
    "        start_time_epoch = time.time()\n",
    "\n",
    "        # Saving the loss\n",
    "        losses.at[epoch, 'running_loss'] = running_loss\n",
    "        losses.at[epoch, 'train_loss'] = train_loss\n",
    "        losses.at[epoch, 'valid_loss'] = validation_loss\n",
    "\n",
    "        # Write on the summary writer\n",
    "        writer.add_scalar('Loss/Train', train_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar('Loss/Validation', validation_loss /\n",
    "                          len(validation_loader), epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', validation_accuracy, epoch)\n",
    "\n",
    "        # Update the learning rate\n",
    "        if lr_scheduler.__class__.__name__ == \"CosineAnnealingWarmRestarts\" and lr_scheduler is not None:\n",
    "            print(f\"\\033[90m--- current LR: {round(lr_scheduler.get_last_lr()[0], 9)} ---\\033[0m\")\n",
    "            lr_scheduler.step()  # step scheduler learning rate\n",
    "\n",
    "        if min_validation_loss > (validation_loss / len(validation_loader)):\n",
    "            print(f'\\033[92m+++ [validation loss decreased ({min_validation_loss:.9f} -> {(validation_loss / len(validation_loader)):.9f}), saving the model ...] +++\\033[0m')\n",
    "            min_validation_loss = validation_loss / len(validation_loader)\n",
    "\n",
    "            # Check if the directory exists, and if not, create it\n",
    "            save_dir = f'./runs/models/{(net.__class__.__name__).lower()}'\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            # Save State Dict\n",
    "            torch.save(net.state_dict(), f'{save_dir}/{(net.__class__.__name__).lower()}_{name}_saved_model.pth')\n",
    "\n",
    "        # Check if early stopping criteria is fulfilled\n",
    "        if early_stopping.early_stop(validation_loss):\n",
    "            break\n",
    "\n",
    "    pickle.dump(losses, open(\n",
    "        f'./runs/models/{(net.__class__.__name__).lower()}/{(net.__class__.__name__).lower()}_{name}_loss.pkl', 'wb'))\n",
    "    writer.close()\n",
    "    print(f\"[i] Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1024]          19,456\n",
      "           Dropout-2                 [-1, 1024]               0\n",
      "            Linear-3                  [-1, 512]         524,800\n",
      "           Dropout-4                  [-1, 512]               0\n",
      "            Linear-5                  [-1, 256]         131,328\n",
      "           Dropout-6                  [-1, 256]               0\n",
      "            Linear-7                  [-1, 128]          32,896\n",
      "           Dropout-8                  [-1, 128]               0\n",
      "            Linear-9                   [-1, 64]           8,256\n",
      "          Dropout-10                   [-1, 64]               0\n",
      "           Linear-11                   [-1, 16]           1,040\n",
      "          Dropout-12                   [-1, 16]               0\n",
      "           Linear-13                    [-1, 6]             102\n",
      "================================================================\n",
      "Total params: 717,878\n",
      "Trainable params: 717,878\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 2.74\n",
      "Estimated Total Size (MB): 2.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleLinearNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleLinearNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.fc4 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.fc5 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.fc6 = nn.Linear(in_features=64, out_features=16)\n",
    "        self.fc7 = nn.Linear(in_features=16, out_features=num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.tanh(self.fc1(x)))\n",
    "        x = self.dropout(F.tanh(self.fc2(x)))\n",
    "        x = self.dropout(F.tanh(self.fc3(x)))\n",
    "        x = self.dropout(F.elu(self.fc4(x)))\n",
    "        x = self.dropout(F.elu(self.fc5(x)))\n",
    "        x = self.dropout(F.elu(self.fc6(x)))\n",
    "        x = F.softmax(self.fc7(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Create and print the model\n",
    "model = SimpleLinearNet(input_size=input_size, num_classes=num_classes).to(device)\n",
    "summary(model, (input_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Traing the network SimpleLinearNet ...\n",
      "+++ [\u001b[1mepoch: 1\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 1, training loss: 1.81378, validation loss: 1.78260] +++\n",
      "--- time-taken for epoch 1: 0.018 seconds ---\n",
      "\u001b[90m--- current LR: 0.003 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (inf -> 1.782599092), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 2\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 2, training loss: 1.77847, validation loss: 1.76801] +++\n",
      "--- time-taken for epoch 2: 0.02 seconds ---\n",
      "\u001b[90m--- current LR: 0.002989063 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.782599092 -> 1.768011451), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 3\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 3, training loss: 1.78685, validation loss: 1.72676] +++\n",
      "--- time-taken for epoch 3: 0.018 seconds ---\n",
      "\u001b[90m--- current LR: 0.002956413 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.768011451 -> 1.726764321), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 4\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 4, training loss: 1.63885, validation loss: 1.68118] +++\n",
      "--- time-taken for epoch 4: 0.015 seconds ---\n",
      "\u001b[90m--- current LR: 0.002902524 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.726764321 -> 1.681182623), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 5\u001b[0m, validation - \u001b[91maccuracy: 0.84375\u001b[0m, \u001b[93mprecision: 0.56757\u001b[0m, \u001b[94mrecall: 0.26250\u001b[0m, \u001b[95mf1-score: 0.35897\u001b[0m] +++\n",
      "+++ [epoch: 5, training loss: 1.62496, validation loss: 1.64145] +++\n",
      "--- time-taken for epoch 5: 0.016 seconds ---\n",
      "\u001b[90m--- current LR: 0.002828184 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.681182623 -> 1.641447425), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 6\u001b[0m, validation - \u001b[91maccuracy: 0.84583\u001b[0m, \u001b[93mprecision: 0.68750\u001b[0m, \u001b[94mrecall: 0.13750\u001b[0m, \u001b[95mf1-score: 0.22917\u001b[0m] +++\n",
      "+++ [epoch: 6, training loss: 1.62943, validation loss: 1.58495] +++\n",
      "--- time-taken for epoch 6: 0.019 seconds ---\n",
      "\u001b[90m--- current LR: 0.002734476 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.641447425 -> 1.584947944), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 7\u001b[0m, validation - \u001b[91maccuracy: 0.85833\u001b[0m, \u001b[93mprecision: 0.57692\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56962\u001b[0m] +++\n",
      "+++ [epoch: 7, training loss: 1.55130, validation loss: 1.54717] +++\n",
      "--- time-taken for epoch 7: 0.016 seconds ---\n",
      "\u001b[90m--- current LR: 0.002622766 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.584947944 -> 1.547172785), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 8\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 8, training loss: 1.47844, validation loss: 1.52783] +++\n",
      "--- time-taken for epoch 8: 0.016 seconds ---\n",
      "\u001b[90m--- current LR: 0.002494684 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.547172785 -> 1.527828574), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 9\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 9, training loss: 1.56942, validation loss: 1.51506] +++\n",
      "--- time-taken for epoch 9: 0.018 seconds ---\n",
      "\u001b[90m--- current LR: 0.002352097 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.527828574 -> 1.515056014), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 10\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 10, training loss: 1.44349, validation loss: 1.50486] +++\n",
      "--- time-taken for epoch 10: 0.014 seconds ---\n",
      "\u001b[90m--- current LR: 0.002197085 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.515056014 -> 1.504861951), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 11\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 11, training loss: 1.46714, validation loss: 1.49601] +++\n",
      "--- time-taken for epoch 11: 0.016 seconds ---\n",
      "\u001b[90m--- current LR: 0.002031907 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.504861951 -> 1.496005297), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 12\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 12, training loss: 1.28479, validation loss: 1.48990] +++\n",
      "--- time-taken for epoch 12: 0.018 seconds ---\n",
      "\u001b[90m--- current LR: 0.001858973 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.496005297 -> 1.489897013), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 13\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 13, training loss: 1.42178, validation loss: 1.48640] +++\n",
      "--- time-taken for epoch 13: 0.018 seconds ---\n",
      "\u001b[90m--- current LR: 0.001680805 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.489897013 -> 1.486403704), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 14\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 14, training loss: 1.25282, validation loss: 1.48425] +++\n",
      "--- time-taken for epoch 14: 0.019 seconds ---\n",
      "\u001b[90m--- current LR: 0.0015 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.486403704 -> 1.484248519), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 15\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 15, training loss: 1.24279, validation loss: 1.48295] +++\n",
      "--- time-taken for epoch 15: 0.015 seconds ---\n",
      "\u001b[90m--- current LR: 0.001319195 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.484248519 -> 1.482946038), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 16\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 16, training loss: 1.37210, validation loss: 1.48228] +++\n",
      "--- time-taken for epoch 16: 0.014 seconds ---\n",
      "\u001b[90m--- current LR: 0.001141027 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.482946038 -> 1.482284427), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 17\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 17, training loss: 1.20563, validation loss: 1.48191] +++\n",
      "--- time-taken for epoch 17: 0.019 seconds ---\n",
      "\u001b[90m--- current LR: 0.000968093 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.482284427 -> 1.481913090), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 18\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 18, training loss: 1.42600, validation loss: 1.48166] +++\n",
      "--- time-taken for epoch 18: 0.017 seconds ---\n",
      "\u001b[90m--- current LR: 0.000802915 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481913090 -> 1.481663227), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 19\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 19, training loss: 1.22591, validation loss: 1.48151] +++\n",
      "--- time-taken for epoch 19: 0.017 seconds ---\n",
      "\u001b[90m--- current LR: 0.000647903 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481663227 -> 1.481512785), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 20\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 20, training loss: 1.51424, validation loss: 1.48142] +++\n",
      "--- time-taken for epoch 20: 0.015 seconds ---\n",
      "\u001b[90m--- current LR: 0.000505316 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481512785 -> 1.481424332), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 21\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 21, training loss: 1.08320, validation loss: 1.48136] +++\n",
      "--- time-taken for epoch 21: 0.016 seconds ---\n",
      "\u001b[90m--- current LR: 0.000377234 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481424332 -> 1.481361270), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 22\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 22, training loss: 1.21158, validation loss: 1.48132] +++\n",
      "--- time-taken for epoch 22: 0.019 seconds ---\n",
      "\u001b[90m--- current LR: 0.000265524 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481361270 -> 1.481322169), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 23\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 23, training loss: 1.38866, validation loss: 1.48130] +++\n",
      "--- time-taken for epoch 23: 0.015 seconds ---\n",
      "\u001b[90m--- current LR: 0.000171816 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481322169 -> 1.481299043), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 24\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 24, training loss: 1.22967, validation loss: 1.48129] +++\n",
      "--- time-taken for epoch 24: 0.014 seconds ---\n",
      "\u001b[90m--- current LR: 9.7476e-05 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481299043 -> 1.481287003), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 25\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 25, training loss: 1.54480, validation loss: 1.48128] +++\n",
      "--- time-taken for epoch 25: 0.015 seconds ---\n",
      "\u001b[90m--- current LR: 4.3587e-05 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481287003 -> 1.481282115), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 26\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 26, training loss: 1.54395, validation loss: 1.48128] +++\n",
      "--- time-taken for epoch 26: 0.016 seconds ---\n",
      "\u001b[90m--- current LR: 1.0937e-05 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481282115 -> 1.481280923), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 27\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 27, training loss: 1.54789, validation loss: 1.48109] +++\n",
      "--- time-taken for epoch 27: 0.02 seconds ---\n",
      "\u001b[90m--- current LR: 0.003 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481280923 -> 1.481090784), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 28\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 28, training loss: 1.38075, validation loss: 1.48103] +++\n",
      "--- time-taken for epoch 28: 0.015 seconds ---\n",
      "\u001b[90m--- current LR: 0.002989063 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481090784 -> 1.481029749), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 29\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 29, training loss: 1.38978, validation loss: 1.48102] +++\n",
      "--- time-taken for epoch 29: 0.015 seconds ---\n",
      "\u001b[90m--- current LR: 0.002956413 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.481029749 -> 1.481015682), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 30\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 30, training loss: 1.37766, validation loss: 1.48102] +++\n",
      "--- time-taken for epoch 30: 0.017 seconds ---\n",
      "\u001b[90m--- current LR: 0.002902524 ---\u001b[0m\n",
      "[i] Validation Loss Increased - Early Stop!\n",
      "--- 1.4810197353363037 > 1.481016682220459 ---\n",
      "[i] Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = SimpleLinearNet(input_size=input_size,num_classes=num_classes).to(device)\n",
    "train_flag = True # Dont run if False\n",
    "name = \"simple_net\"\n",
    "\n",
    "if train_flag:\n",
    "    print(f\"[i] Traing the network {net.__class__.__name__} ...\")\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate = 3e-3\n",
    "    \n",
    "    # Define your criterion (e.g., CrossEntropyLoss for multiclass classification)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Define your optimizer (e.g., Adam)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Cosine Annealing with Restarts (CWR) scheduler.\n",
    "    # This scheduler is designed to automatically adjust the learning rate according to a cosine wave, and can be used to adjust the learning rate as the model converges.\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=round((1/10)*batch_size), T_mult=1, eta_min=0)\n",
    "\n",
    "    # Train the model\n",
    "    train(net, train_dataloader, val_dataloader, num_epochs, batch_size, mini_batch_size, optimizer, scheduler, criterion, name=name)\n",
    "else: # load the model\n",
    "    print(f\"[i] Loading the network {net.__class__.__name__} ...\")\n",
    "    #Loading existing models (with saved weights)\n",
    "    net.load_state_dict(torch.load(f'./runs/models/{(net.__class__.__name__).lower()}/{(net.__class__.__name__).lower()}_{name}_saved_model.pth', map_location=device)) #using saved data if present\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]           4,864\n",
      "       BatchNorm1d-2                  [-1, 256]             512\n",
      "           Dropout-3                  [-1, 256]               0\n",
      "            Linear-4                  [-1, 128]          32,896\n",
      "       BatchNorm1d-5                  [-1, 128]             256\n",
      "           Dropout-6                  [-1, 128]               0\n",
      "            Linear-7                   [-1, 64]           8,256\n",
      "       BatchNorm1d-8                   [-1, 64]             128\n",
      "           Dropout-9                   [-1, 64]               0\n",
      "           Linear-10                   [-1, 32]           2,080\n",
      "      BatchNorm1d-11                   [-1, 32]              64\n",
      "          Dropout-12                   [-1, 32]               0\n",
      "           Linear-13                   [-1, 16]             528\n",
      "      BatchNorm1d-14                   [-1, 16]              32\n",
      "          Dropout-15                   [-1, 16]               0\n",
      "           Linear-16                    [-1, 8]             136\n",
      "      BatchNorm1d-17                    [-1, 8]              16\n",
      "          Dropout-18                    [-1, 8]               0\n",
      "           Linear-19                    [-1, 6]              54\n",
      "================================================================\n",
      "Total params: 49,822\n",
      "Trainable params: 49,822\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 0.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class MoreComplexNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, init_fn):\n",
    "        super(MoreComplexNet, self).__init__()\n",
    "        self.init_fn = init_fn\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.fc5 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.bn5 = nn.BatchNorm1d(16)\n",
    "\n",
    "        self.fc6 = nn.Linear(in_features=16, out_features=8)\n",
    "        self.bn6 = nn.BatchNorm1d(8)\n",
    "\n",
    "        self.fc7 = nn.Linear(in_features=8, out_features=num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            self.init_fn(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.tanh(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout(F.elu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout(F.elu(self.bn3(self.fc3(x))))\n",
    "        x = self.dropout(F.elu(self.bn4(self.fc4(x))))\n",
    "        x = self.dropout(F.elu(self.bn5(self.fc5(x))))\n",
    "        x = self.dropout(F.elu(self.bn6(self.fc6(x))))\n",
    "        x = F.softmax(self.fc7(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "# Create and print the model\n",
    "model = MoreComplexNet(input_size, num_classes=num_classes, init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "summary(model, (input_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Traing the network MoreComplexNet ...\n",
      "+++ [\u001b[1mepoch: 1\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 1, training loss: 1.72439, validation loss: 1.77888] +++\n",
      "--- time-taken for epoch 1: 0.013 seconds ---\n",
      "\u001b[90m--- current LR: 0.003 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (inf -> 1.778876901), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 2\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 2, training loss: 1.77008, validation loss: 1.76433] +++\n",
      "--- time-taken for epoch 2: 0.015 seconds ---\n",
      "\u001b[90m--- current LR: 0.002989063 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.778876901 -> 1.764332533), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 3\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 3, training loss: 1.74825, validation loss: 1.74813] +++\n",
      "--- time-taken for epoch 3: 0.016 seconds ---\n",
      "\u001b[90m--- current LR: 0.002956413 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.764332533 -> 1.748132467), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 4\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 4, training loss: 1.74888, validation loss: 1.72712] +++\n",
      "--- time-taken for epoch 4: 0.016 seconds ---\n",
      "\u001b[90m--- current LR: 0.002902524 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.748132467 -> 1.727115989), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 5\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 5, training loss: 1.82550, validation loss: 1.71873] +++\n",
      "--- time-taken for epoch 5: 0.013 seconds ---\n",
      "\u001b[90m--- current LR: 0.002828184 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.727115989 -> 1.718728304), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 6\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 6, training loss: 1.80405, validation loss: 1.70323] +++\n",
      "--- time-taken for epoch 6: 0.012 seconds ---\n",
      "\u001b[90m--- current LR: 0.002734476 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.718728304 -> 1.703228354), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 7\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 7, training loss: 1.85275, validation loss: 1.71045] +++\n",
      "--- time-taken for epoch 7: 0.014 seconds ---\n",
      "\u001b[90m--- current LR: 0.002622766 ---\u001b[0m\n",
      "[i] Validation Loss Increased - Early Stop!\n",
      "--- 1.710445761680603 > 1.7032293544540404 ---\n",
      "[i] Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = MoreComplexNet(input_dim=input_size, num_classes=num_classes,\n",
    "                     init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "train_flag = True\n",
    "name = \"more_complex_net\"\n",
    "\n",
    "if train_flag:\n",
    "    print(f\"[i] Traing the network {net.__class__.__name__} ...\")\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate = 3e-3\n",
    "    \n",
    "    # Define your criterion (e.g., CrossEntropyLoss for multiclass classification)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Define your optimizer (e.g., Adam)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Cosine Annealing with Restarts (CWR) scheduler.\n",
    "    # This scheduler is designed to automatically adjust the learning rate according to a cosine wave, and can be used to adjust the learning rate as the model converges.\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=round((1/10)*batch_size), T_mult=1, eta_min=0)\n",
    "\n",
    "    # Train the model\n",
    "    train(net, train_dataloader, val_dataloader, num_epochs, batch_size, mini_batch_size, optimizer, scheduler, criterion, name=name)\n",
    "else: # load the model\n",
    "    print(f\"[i] Loading the network {net.__class__.__name__} ...\")\n",
    "    #Loading existing models (with saved weights)\n",
    "    net.load_state_dict(torch.load(f'./runs/models/{(net.__class__.__name__).lower()}/{(net.__class__.__name__).lower()}_{name}_saved_model.pth', map_location=device)) #using saved data if present\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1024]          19,456\n",
      "       BatchNorm1d-2                 [-1, 1024]           2,048\n",
      "           Dropout-3                 [-1, 1024]               0\n",
      "            Linear-4                  [-1, 512]         524,800\n",
      "       BatchNorm1d-5                  [-1, 512]           1,024\n",
      "           Dropout-6                  [-1, 512]               0\n",
      "            Linear-7                  [-1, 256]         131,328\n",
      "       BatchNorm1d-8                  [-1, 256]             512\n",
      "           Dropout-9                  [-1, 256]               0\n",
      "           Linear-10                  [-1, 128]          32,896\n",
      "      BatchNorm1d-11                  [-1, 128]             256\n",
      "          Dropout-12                  [-1, 128]               0\n",
      "           Linear-13                   [-1, 64]           8,256\n",
      "      BatchNorm1d-14                   [-1, 64]             128\n",
      "          Dropout-15                   [-1, 64]               0\n",
      "           Linear-16                   [-1, 32]           2,080\n",
      "      BatchNorm1d-17                   [-1, 32]              64\n",
      "          Dropout-18                   [-1, 32]               0\n",
      "           Linear-19                   [-1, 16]             528\n",
      "      BatchNorm1d-20                   [-1, 16]              32\n",
      "          Dropout-21                   [-1, 16]               0\n",
      "           Linear-22                    [-1, 8]             136\n",
      "      BatchNorm1d-23                    [-1, 8]              16\n",
      "          Dropout-24                    [-1, 8]               0\n",
      "           Linear-25                    [-1, 6]              54\n",
      "================================================================\n",
      "Total params: 723,614\n",
      "Trainable params: 723,614\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.05\n",
      "Params size (MB): 2.76\n",
      "Estimated Total Size (MB): 2.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class EvenMoreComplexNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, init_fn):\n",
    "        super(EvenMoreComplexNet, self).__init__()\n",
    "        self.init_fn = init_fn\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=1024)\n",
    "        self.bn1 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.fc4 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc5 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.fc6 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.bn6 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.fc7 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.bn7 = nn.BatchNorm1d(16)\n",
    "\n",
    "        self.fc8 = nn.Linear(in_features=16, out_features=8)\n",
    "        self.bn8 = nn.BatchNorm1d(8)\n",
    "\n",
    "        self.fc_out = nn.Linear(in_features=8, out_features=num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            self.init_fn(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.tanh(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout(F.elu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout(F.elu(self.bn3(self.fc3(x))))\n",
    "        x = self.dropout(F.elu(self.bn4(self.fc4(x))))\n",
    "        x = self.dropout(F.elu(self.bn5(self.fc5(x))))\n",
    "        x = self.dropout(F.elu(self.bn6(self.fc6(x))))\n",
    "        x = self.dropout(F.elu(self.bn7(self.fc7(x))))\n",
    "        x = self.dropout(F.elu(self.bn8(self.fc8(x))))\n",
    "        x = F.softmax(self.fc_out(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Create and print the model\n",
    "model = EvenMoreComplexNet(input_size, num_classes=num_classes, init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "summary(model, (input_size,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Traing the network EvenMoreComplexNet ...\n",
      "+++ [\u001b[1mepoch: 1\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 1, training loss: 1.82581, validation loss: 1.71995] +++\n",
      "--- time-taken for epoch 1: 0.021 seconds ---\n",
      "\u001b[90m--- current LR: 0.003 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (inf -> 1.719954848), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 2\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 2, training loss: 1.84712, validation loss: 1.71481] +++\n",
      "--- time-taken for epoch 2: 0.028 seconds ---\n",
      "\u001b[90m--- current LR: 0.002989063 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.719954848 -> 1.714809179), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 3\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 3, training loss: 1.76775, validation loss: 1.71247] +++\n",
      "--- time-taken for epoch 3: 0.044 seconds ---\n",
      "\u001b[90m--- current LR: 0.002956413 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.714809179 -> 1.712471962), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 4\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 4, training loss: 1.81045, validation loss: 1.69937] +++\n",
      "--- time-taken for epoch 4: 0.033 seconds ---\n",
      "\u001b[90m--- current LR: 0.002902524 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.712471962 -> 1.699370980), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 5\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 5, training loss: 1.77181, validation loss: 1.69821] +++\n",
      "--- time-taken for epoch 5: 0.022 seconds ---\n",
      "\u001b[90m--- current LR: 0.002828184 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.699370980 -> 1.698214293), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 6\u001b[0m, validation - \u001b[91maccuracy: 0.83333\u001b[0m, \u001b[93mprecision: 0.00000\u001b[0m, \u001b[94mrecall: 0.00000\u001b[0m, \u001b[95mf1-score: 0.00000\u001b[0m] +++\n",
      "+++ [epoch: 6, training loss: 1.84009, validation loss: 1.70427] +++\n",
      "--- time-taken for epoch 6: 0.022 seconds ---\n",
      "\u001b[90m--- current LR: 0.002734476 ---\u001b[0m\n",
      "[i] Validation Loss Increased - Early Stop!\n",
      "--- 1.7042694091796875 > 1.698215292526245 ---\n",
      "[i] Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = EvenMoreComplexNet(input_dim=input_size, num_classes=num_classes,\n",
    "                     init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "train_flag = True\n",
    "name = \"even_more_complex_net\"\n",
    "\n",
    "if train_flag:\n",
    "    print(f\"[i] Traing the network {net.__class__.__name__} ...\")\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate = 3e-3\n",
    "    \n",
    "    # Define your criterion (e.g., CrossEntropyLoss for multiclass classification)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Define your optimizer (e.g., Adam)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Cosine Annealing with Restarts (CWR) scheduler.\n",
    "    # This scheduler is designed to automatically adjust the learning rate according to a cosine wave, and can be used to adjust the learning rate as the model converges.\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=round((1/10)*batch_size), T_mult=1, eta_min=0)\n",
    "\n",
    "    # Train the model\n",
    "    train(net, train_dataloader, val_dataloader, num_epochs, batch_size, mini_batch_size, optimizer, scheduler, criterion, name=name)\n",
    "else: # load the model\n",
    "    print(f\"[i] Loading the network {net.__class__.__name__} ...\")\n",
    "    #Loading existing models (with saved weights)\n",
    "    net.load_state_dict(torch.load(f'./runs/models/{(net.__class__.__name__).lower()}/{(net.__class__.__name__).lower()}_{name}_saved_model.pth', map_location=device)) #using saved data if present\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 512]           9,728\n",
      "           Dropout-2                  [-1, 512]               0\n",
      "            Linear-3                  [-1, 512]         262,656\n",
      "           Dropout-4                  [-1, 512]               0\n",
      "            Linear-5                    [-1, 6]           3,078\n",
      "================================================================\n",
      "Total params: 275,462\n",
      "Trainable params: 275,462\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 1.05\n",
      "Estimated Total Size (MB): 1.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SimplestNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, init_fn):\n",
    "        super(SimplestNet, self).__init__()\n",
    "\n",
    "        self.init_fn = init_fn\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            self.init_fn(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "# Create and print the model\n",
    "model = SimplestNet(input_size=input_size, hidden_size=512, num_classes=num_classes, init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "summary(model, (input_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Traing the network SimplestNet ...\n",
      "+++ [\u001b[1mepoch: 1\u001b[0m, validation - \u001b[91maccuracy: 0.86667\u001b[0m, \u001b[93mprecision: 0.90000\u001b[0m, \u001b[94mrecall: 0.22500\u001b[0m, \u001b[95mf1-score: 0.36000\u001b[0m] +++\n",
      "+++ [epoch: 1, training loss: 1.78566, validation loss: 1.64003] +++\n",
      "--- time-taken for epoch 1: 0.01 seconds ---\n",
      "\u001b[90m--- current LR: 0.003 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (inf -> 1.640026331), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 2\u001b[0m, validation - \u001b[91maccuracy: 0.86042\u001b[0m, \u001b[93mprecision: 0.58442\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.57325\u001b[0m] +++\n",
      "+++ [epoch: 2, training loss: 1.52439, validation loss: 1.52111] +++\n",
      "--- time-taken for epoch 2: 0.013 seconds ---\n",
      "\u001b[90m--- current LR: 0.002989063 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.640026331 -> 1.521108866), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 3\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 3, training loss: 1.33653, validation loss: 1.48051] +++\n",
      "--- time-taken for epoch 3: 0.012 seconds ---\n",
      "\u001b[90m--- current LR: 0.002956413 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.521108866 -> 1.480507970), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 4\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 4, training loss: 1.38534, validation loss: 1.47787] +++\n",
      "--- time-taken for epoch 4: 0.012 seconds ---\n",
      "\u001b[90m--- current LR: 0.002902524 ---\u001b[0m\n",
      "\u001b[92m+++ [validation loss decreased (1.480507970 -> 1.477866888), saving the model ...] +++\u001b[0m\n",
      "+++ [\u001b[1mepoch: 5\u001b[0m, validation - \u001b[91maccuracy: 0.85417\u001b[0m, \u001b[93mprecision: 0.56250\u001b[0m, \u001b[94mrecall: 0.56250\u001b[0m, \u001b[95mf1-score: 0.56250\u001b[0m] +++\n",
      "+++ [epoch: 5, training loss: 1.35511, validation loss: 1.47937] +++\n",
      "--- time-taken for epoch 5: 0.01 seconds ---\n",
      "\u001b[90m--- current LR: 0.002828184 ---\u001b[0m\n",
      "[i] Validation Loss Increased - Early Stop!\n",
      "--- 1.479371428489685 > 1.4778678880462646 ---\n",
      "[i] Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = SimplestNet(input_size=input_size, hidden_size=512, num_classes=num_classes, init_fn=torch.nn.init.xavier_normal_).to(device)\n",
    "train_flag = True\n",
    "name = \"simplest_net\"\n",
    "\n",
    "if train_flag:\n",
    "    print(f\"[i] Traing the network {net.__class__.__name__} ...\")\n",
    "\n",
    "    # Learning rate\n",
    "    learning_rate = 3e-3\n",
    "    \n",
    "    # Define your criterion (e.g., CrossEntropyLoss for multiclass classification)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Define your optimizer (e.g., Adam)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Cosine Annealing with Restarts (CWR) scheduler.\n",
    "    # This scheduler is designed to automatically adjust the learning rate according to a cosine wave, and can be used to adjust the learning rate as the model converges.\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=round((1/10)*batch_size), T_mult=1, eta_min=0)\n",
    "\n",
    "    # Train the model\n",
    "    train(net, train_dataloader, val_dataloader, num_epochs, batch_size, mini_batch_size, optimizer, scheduler, criterion, name=name)\n",
    "else: # load the model\n",
    "    print(f\"[i] Loading the network {net.__class__.__name__} ...\")\n",
    "    #Loading existing models (with saved weights)\n",
    "    net.load_state_dict(torch.load(f'./runs/models/{(net.__class__.__name__).lower()}/{(net.__class__.__name__).lower()}_{name}_saved_model.pth', map_location=device)) #using saved data if present\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    actual_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Map the predicted class indices to class labels\n",
    "            predicted_labels.append(np.argmax(outputs.cpu().numpy(), axis=1))\n",
    "            actual_labels.append(np.argmax(labels.cpu().numpy(), axis=1))\n",
    "\n",
    "    return actual_labels, predicted_labels\n",
    "\n",
    "# Evaluate the model using the test dataset\n",
    "actual_labels, predicted_lables = evaluate_model(net, test_dataloader)\n",
    "\n",
    "actual_labels_flat = [item for sublist in actual_labels for item in sublist]\n",
    "predicted_labels_flat = [item for sublist in predicted_lables for item in sublist]\n",
    "\n",
    "conf_matrix = confusion_matrix(actual_labels_flat, predicted_labels_flat, normalize='pred')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48 , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.095, 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.095, 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.25 , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.01 , 0.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.07 , 0.   , 0.   , 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(actual_labels, predicted_lables)\n",
    "precision = precision_score(actual_labels, predicted_lables, average='weighted', zero_division=1)\n",
    "recall = recall_score(actual_labels, predicted_lables, average='weighted')\n",
    "f1 = f1_score(actual_labels, predicted_lables, average='weighted')\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', xticklabels=np.unique(actual_labels), yticklabels=np.unique(actual_labels))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Print overall performance metrics\n",
    "print(\"Overall Metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "# Individual class metrics\n",
    "class_metrics = {'class': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1_score': []}\n",
    "for label in np.unique(actual_labels):\n",
    "    class_accuracy = accuracy_score(actual_labels == label, predicted_lables == label)\n",
    "    class_precision = precision_score(actual_labels == label, predicted_lables == label, zero_division=0)\n",
    "    class_recall = recall_score(actual_labels == label, predicted_lables == label)\n",
    "    class_f1 = f1_score(actual_labels == label, predicted_lables == label)\n",
    "\n",
    "    class_metrics['class'].append(label)\n",
    "    class_metrics['accuracy'].append(class_accuracy)\n",
    "    class_metrics['precision'].append(class_precision)\n",
    "    class_metrics['recall'].append(class_recall)\n",
    "    class_metrics['f1_score'].append(class_f1)\n",
    "\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "class_metrics_df = pd.DataFrame(class_metrics)\n",
    "print(\"\\nIndividual Class Metrics:\\n\", class_metrics_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
